{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../')\n",
    "os.environ['SEQ_SPLITS_DATA_PATH'] = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.mlspace/envs/splits/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from runs.train import prepare_data, create_dataloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_133878/2952577576.py:3: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  with initialize(config_path=\"../runs/configs/\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda_visible_devices: 0\n",
      "random_state: 101\n",
      "clearml_project_folder: null\n",
      "clearml_task_name: null\n",
      "use_pretrained_embeddings: false\n",
      "pretrained_embeddings:\n",
      "  add_padding_emb: true\n",
      "  freeze: false\n",
      "use_semantic_ids: true\n",
      "semantic_ids_map_path: /home/jovyan/gusak/semantic_seqrec/data/item_sem_id_modified.pkl\n",
      "semantic_ids_len: 4\n",
      "split_type: global_timesplit\n",
      "split_subtype: val_by_time\n",
      "quantile: 0.9\n",
      "validation_quantile: 0.9\n",
      "dataset_params:\n",
      "  max_length: 128\n",
      "dataloader:\n",
      "  batch_size: 128\n",
      "  test_batch_size: 256\n",
      "  num_workers: 8\n",
      "  validation_size: 2048\n",
      "seqrec_module:\n",
      "  lr: 0.001\n",
      "  predict_top_k: 10\n",
      "  filter_seen: false\n",
      "trainer_params:\n",
      "  max_epochs: 10\n",
      "  accelerator: gpu\n",
      "patience: 20\n",
      "load_if_possible: false\n",
      "evaluator:\n",
      "  successive_val: false\n",
      "  successive_test: false\n",
      "  successive_test_retrained: false\n",
      "  calc_successive_metrics_val: true\n",
      "  calc_successive_metrics_test: true\n",
      "  calc_successive_metrics_test_retrained: true\n",
      "  successive_replay_metrics: false\n",
      "  metrics:\n",
      "  - NDCG\n",
      "  - HitRate\n",
      "  - MRR\n",
      "  - Coverage\n",
      "  top_k:\n",
      "  - 5\n",
      "  - 10\n",
      "  - 20\n",
      "retrain_with_validation: false\n",
      "save_val_last_predictions: false\n",
      "save_test_last_predictions: false\n",
      "dataset:\n",
      "  name: Beauty\n",
      "  filter_seen: true\n",
      "  column_name:\n",
      "    user_id: user_id\n",
      "    item_id: item_id\n",
      "    timestamp: timestamp\n",
      "    relevance: null\n",
      "  pretrained_embeddings_path: data/embeddings/my_beauty_embs.npy\n",
      "model:\n",
      "  model_class: GPT-2\n",
      "  pdrop: 0.1\n",
      "  model_params:\n",
      "    n_positions: 128\n",
      "    n_embd: 64\n",
      "    n_layer: 2\n",
      "    n_head: 1\n",
      "    embd_pdrop: 0.1\n",
      "    attn_pdrop: 0.1\n",
      "  generation: true\n",
      "  mode: greedy\n",
      "  generation_params:\n",
      "    do_sample: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = OmegaConf.load('../runs/configs/train.yaml')\n",
    "\n",
    "with initialize(config_path=\"../runs/configs/\"):  \n",
    "    config = compose(\n",
    "        config_name=\"train\",      \n",
    "        overrides=[\n",
    "            \"quantile=0.9\",\n",
    "            \"split_subtype=val_by_time\",\n",
    "            \"dataset=Beauty\",\n",
    "            \"model=GPT2\",\n",
    "            \"trainer_params.max_epochs=10\",\n",
    "            \"use_semantic_ids=True\",\n",
    "            \"semantic_ids_map_path=/home/jovyan/gusak/semantic_seqrec/data/item_sem_id_modified.pkl\",\n",
    "            \"model.mode=greedy\",\n",
    "            \n",
    "        ],\n",
    "        return_hydra_config=False,\n",
    "    )\n",
    "\n",
    "print(OmegaConf.to_yaml(config, resolve=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.cuda_visible_devices = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import hydra\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from clearml import Task\n",
    "from omegaconf import OmegaConf\n",
    "from pytorch_lightning import seed_everything\n",
    "from pytorch_lightning.callbacks import (EarlyStopping, ModelCheckpoint,\n",
    "                                         ModelSummary, TQDMProgressBar)\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.datasets import (CausalLMDataset, CausalLMPredictionDataset,\n",
    "                          PaddingCollateFn)\n",
    "from src.metrics import Evaluator\n",
    "from src.models import SASRec\n",
    "from src.modules import SeqRec, SeqRecHuggingface\n",
    "from src.postprocess import preds2recs\n",
    "from src.prepr import last_item_split\n",
    "\n",
    "import itertools\n",
    "import pickle\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape (160178, 4)\n",
      "validation shape (66297, 4)\n",
      "test shape (70263, 4)\n",
      "   user_id               item_id   timestamp            user_id_old\n",
      "0        1  [117, 318, 621, 768]  1357430400  A00473363TJ8YSZ3YAGG9\n",
      "1        1  [114, 386, 594, 768]  1384387200  A00473363TJ8YSZ3YAGG9\n",
      "2        1    [1, 470, 745, 768]  1384387200  A00473363TJ8YSZ3YAGG9\n",
      "3        1   [91, 411, 726, 768]  1387843200  A00473363TJ8YSZ3YAGG9\n",
      "4        2   [6, 377, 571, 1013]  1385337600  A00700212KB3K0MVESPIY\n",
      "   user_id              item_id   timestamp            user_id_old\n",
      "0        0  [67, 376, 750, 768]  1405296000  A00414041RD0BXM6WK0GX\n",
      "1        0  [67, 390, 715, 768]  1405296000  A00414041RD0BXM6WK0GX\n",
      "2        0  [67, 313, 751, 768]  1405296000  A00414041RD0BXM6WK0GX\n",
      "3        0  [67, 365, 605, 768]  1405296000  A00414041RD0BXM6WK0GX\n",
      "4        0  [67, 482, 721, 768]  1405296000  A00414041RD0BXM6WK0GX\n",
      "   user_id               item_id   timestamp            user_id_old\n",
      "0        1  [117, 318, 621, 768]  1357430400  A00473363TJ8YSZ3YAGG9\n",
      "1        1  [114, 386, 594, 768]  1384387200  A00473363TJ8YSZ3YAGG9\n",
      "2        1    [1, 470, 745, 768]  1384387200  A00473363TJ8YSZ3YAGG9\n",
      "3        1   [91, 411, 726, 768]  1387843200  A00473363TJ8YSZ3YAGG9\n",
      "4        1    [1, 302, 677, 769]  1399593600  A00473363TJ8YSZ3YAGG9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id item_id   timestamp            user_id_old\n",
      "0        1     117  1357430400  A00473363TJ8YSZ3YAGG9\n",
      "0        1     318  1357430400  A00473363TJ8YSZ3YAGG9\n",
      "0        1     621  1357430400  A00473363TJ8YSZ3YAGG9\n",
      "0        1     768  1357430400  A00473363TJ8YSZ3YAGG9\n",
      "1        1     114  1384387200  A00473363TJ8YSZ3YAGG9\n",
      "   user_id item_id   timestamp            user_id_old\n",
      "0        0      67  1405296000  A00414041RD0BXM6WK0GX\n",
      "0        0     376  1405296000  A00414041RD0BXM6WK0GX\n",
      "0        0     750  1405296000  A00414041RD0BXM6WK0GX\n",
      "0        0     768  1405296000  A00414041RD0BXM6WK0GX\n",
      "1        0      67  1405296000  A00414041RD0BXM6WK0GX\n",
      "   user_id item_id   timestamp            user_id_old\n",
      "0        1     117  1357430400  A00473363TJ8YSZ3YAGG9\n",
      "0        1     318  1357430400  A00473363TJ8YSZ3YAGG9\n",
      "0        1     621  1357430400  A00473363TJ8YSZ3YAGG9\n",
      "0        1     768  1357430400  A00473363TJ8YSZ3YAGG9\n",
      "1        1     114  1384387200  A00473363TJ8YSZ3YAGG9\n",
      "Test global timepoint 1399939200.0\n",
      "Validation global timepoint 1394668800.0\n"
     ]
    }
   ],
   "source": [
    "if hasattr(config, 'cuda_visible_devices'):\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(config.cuda_visible_devices)\n",
    "\n",
    "\n",
    "train, validation, test, max_item_id, global_timepoint, global_timepoint_val = prepare_data(config)\n",
    "\n",
    "train_loader, eval_loader = create_dataloaders(train, validation, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1706"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_item_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[162, 466, 580,  ...,   0,   0,   0],\n",
      "        [120, 460, 664,  ...,   0,   0,   0],\n",
      "        [128, 407, 661,  ...,   0,   0,   0],\n",
      "        ...,\n",
      "        [ 75, 400, 715,  ...,   0,   0,   0],\n",
      "        [167, 419, 751,  ...,   0,   0,   0],\n",
      "        [221, 495, 615,  ...,   0,   0,   0]]), 'user_id': tensor([   4,   11,   24,   28,   42,   47,   48,   69,   75,   82,   90,   93,\n",
      "         102,  115,  123,  130,  133,  138,  144,  153,  168,  184,  191,  192,\n",
      "         196,  198,  200,  201,  205,  206,  221,  229,  256,  259,  263,  268,\n",
      "         278,  279,  294,  295,  297,  298,  334,  349,  351,  354,  365,  370,\n",
      "         381,  386,  387,  424,  450,  454,  468,  469,  484,  486,  497,  506,\n",
      "         528,  539,  545,  570,  580,  594,  600,  610,  616,  617,  628,  643,\n",
      "         652,  661,  663,  678,  688,  702,  718,  720,  737,  748,  769,  792,\n",
      "         806,  829,  830,  831,  835,  881,  884,  890,  891,  901,  915,  916,\n",
      "         920,  921,  924,  926,  932,  946,  953,  979,  980,  994, 1010, 1011,\n",
      "        1044, 1060, 1064, 1073, 1114, 1126, 1132, 1138, 1140, 1143, 1160, 1170,\n",
      "        1173, 1182, 1196, 1215, 1229, 1242, 1251, 1252, 1256, 1269, 1281, 1282,\n",
      "        1283, 1298, 1304, 1308, 1323, 1338, 1370, 1376, 1388, 1390, 1399, 1421,\n",
      "        1425, 1427, 1464, 1477, 1483, 1485, 1497, 1499, 1505, 1524, 1536, 1566,\n",
      "        1574, 1586, 1589, 1590, 1607, 1608, 1610, 1652, 1657, 1660, 1686, 1697,\n",
      "        1712, 1732, 1741, 1753, 1768, 1769, 1824, 1835, 1864, 1875, 1926, 1957,\n",
      "        1974, 1998, 2003, 2010, 2014, 2020, 2037, 2046, 2050, 2062, 2097, 2120,\n",
      "        2129, 2137, 2141, 2143, 2148, 2160, 2164, 2167, 2181, 2192, 2208, 2233,\n",
      "        2245, 2249, 2250, 2285, 2290, 2295, 2296, 2328, 2342, 2360, 2362, 2370,\n",
      "        2371, 2385, 2389, 2402, 2413, 2427, 2431, 2445, 2448, 2480, 2485, 2489,\n",
      "        2491, 2511, 2513, 2515, 2532, 2534, 2551, 2564, 2566, 2573, 2577, 2582,\n",
      "        2585, 2586, 2595, 2597, 2614, 2636, 2663, 2677, 2678, 2732, 2765, 2777,\n",
      "        2780, 2782, 2783, 2798]), 'seen_ids': tensor([[162, 466, 580,  ...,   0,   0,   0],\n",
      "        [120, 460, 664,  ...,   0,   0,   0],\n",
      "        [128, 407, 661,  ...,   0,   0,   0],\n",
      "        ...,\n",
      "        [ 75, 400, 715,  ...,   0,   0,   0],\n",
      "        [167, 419, 751,  ...,   0,   0,   0],\n",
      "        [221, 495, 615,  ...,   0,   0,   0]]), 'target': tensor([[110, 393, 628, 768],\n",
      "        [ 53, 261, 585, 768],\n",
      "        [135, 386, 589, 768],\n",
      "        ...,\n",
      "        [126, 378, 609, 768],\n",
      "        [240, 360, 765, 768],\n",
      "        [104, 450, 701, 768]]), 'attention_mask': tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.]])}\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(eval_loader))\n",
    "\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/gusak/semantic_seqrec/notebooks'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from runs.train import create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.model.model_params.n_positions = config.dataset_params.max_length * config.semantic_ids_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.model.model_params.n_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(config, item_count=max_item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(1707, 64)\n",
       "    (wpe): Embedding(512, 64)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-1): 2 x GPT2Block(\n",
       "        (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=192, nx=64)\n",
       "          (c_proj): Conv1D(nf=64, nx=64)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=256, nx=64)\n",
       "          (c_proj): Conv1D(nf=64, nx=256)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=64, out_features=1707, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/gusak/semantic_seqrec/models/global_timesplit/val_by_time/Beauty/q09/GPT-2/\n"
     ]
    }
   ],
   "source": [
    "retrain = False\n",
    "split_subtype = config.split_subtype or ''\n",
    "q = 'q0' + str(config.quantile)[2:] if config.split_type == 'global_timesplit' else ''\n",
    "model_path = os.path.join(\n",
    "    os.path.dirname(os.path.abspath('.')), 'models', config.split_type,\n",
    "    split_subtype, config.dataset.name, q, config.model.model_class, 'retrain_with_val' if retrain else '')\n",
    "\n",
    "print(model_path)\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.trainer_params.accelerator = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "if config.model.model_class == 'SASRec':\n",
    "    file_name = (\n",
    "        f\"{config.model.model_params.hidden_units}_\"\n",
    "        f\"{config.model.model_params.num_blocks}_\"\n",
    "        f\"{config.model.model_params.num_heads}_\"\n",
    "        f\"{config.model.model_params.dropout_rate}_\"\n",
    "        f\"{config.model.model_params.maxlen}_\"\n",
    "        f\"{config.dataloader.batch_size}_\"\n",
    "        f\"{config.random_state}\"\n",
    "    )\n",
    "elif config.model.model_class == 'GPT-2':\n",
    "    file_name = (\n",
    "        f\"{config.model.model_params.n_embd}_\"\n",
    "        f\"{config.model.model_params.n_layer}_\"\n",
    "        f\"{config.model.model_params.n_head}_\"\n",
    "        f\"{config.dataloader.batch_size}_\"\n",
    "        f\"{config.random_state}\"\n",
    "    )\n",
    "\n",
    "checkpoint_file = os.path.join(model_path, file_name + \".ckpt\")\n",
    "\n",
    "if config.model.model_class == 'GPT-2':\n",
    "    seqrec_module = SeqRecHuggingface(model, **config['seqrec_module'])\n",
    "    if config.model.generation:\n",
    "        with open(config.semantic_ids_map_path, 'rb') as f:\n",
    "            index2semid = pickle.load(f)\n",
    "        inv_map = {tuple(sem_ids): item_id for item_id, sem_ids in index2semid.items()}\n",
    "        seqrec_module.set_predict_mode(generate=True, mode=config.model.mode,\n",
    "                                        N=config.semantic_ids_len,\n",
    "                                        inv_map=inv_map,\n",
    "                                        **config.model.generation_params)\n",
    "else:   \n",
    "    seqrec_module = SeqRec(model, **config['seqrec_module']) \n",
    "\n",
    "model_summary = ModelSummary(max_depth=1)\n",
    "progress_bar = TQDMProgressBar(refresh_rate=20)\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    dirpath=model_path,  \n",
    "    filename='_' + file_name,           \n",
    "    save_top_k=1,\n",
    "    monitor=\"val_ndcg\",\n",
    "    mode=\"max\",\n",
    "    save_weights_only=True\n",
    ")\n",
    "early_stopping = EarlyStopping(monitor=\"val_ndcg\", mode=\"max\",\n",
    "                            patience=config.patience, verbose=False)\n",
    "callbacks = [early_stopping, model_summary, checkpoint, progress_bar]\n",
    "\n",
    "trainer = pl.Trainer(callbacks=callbacks, enable_checkpointing=True, \n",
    "                        **config['trainer_params'])\n",
    "\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 37, 371, 559,  ...,   0,   0,   0],\n",
       "         [  1, 407, 520,  ...,   0,   0,   0],\n",
       "         [239, 408, 644,  ...,   0,   0,   0],\n",
       "         ...,\n",
       "         [211, 470, 548,  ...,   0,   0,   0],\n",
       "         [236, 280, 648,  ...,   0,   0,   0],\n",
       "         [ 83, 408, 701,  ...,   0,   0,   0]]),\n",
       " 'labels': tensor([[  37,  371,  559,  ..., -100, -100, -100],\n",
       "         [   1,  407,  520,  ..., -100, -100, -100],\n",
       "         [ 239,  408,  644,  ..., -100, -100, -100],\n",
       "         ...,\n",
       "         [ 211,  470,  548,  ..., -100, -100, -100],\n",
       "         [ 236,  280,  648,  ..., -100, -100, -100],\n",
       "         [  83,  408,  701,  ..., -100, -100, -100]]),\n",
       " 'attention_mask': tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.]])}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  0,   0,   0,  ..., 316, 677, 768],\n",
       "         [  0,   0,   0,  ..., 269, 696, 768],\n",
       "         [  0,   0,   0,  ..., 345, 520, 768],\n",
       "         ...,\n",
       "         [  0,   0,   0,  ..., 313, 669, 768],\n",
       "         [  0,   0,   0,  ..., 346, 566, 768],\n",
       "         [  0,   0,   0,  ..., 295, 644, 768]]),\n",
       " 'user_id': tensor([  22,   24,   27,   44,   46,   47,   55,   63,   66,   69,  102,  115,\n",
       "          140,  144,  152,  159,  184,  192,  194,  198,  205,  206,  241,  248,\n",
       "          249,  258,  259,  275,  278,  279,  285,  287,  289,  294,  295,  298,\n",
       "          303,  311,  334,  354,  359,  360,  381,  386,  394,  396,  402,  404,\n",
       "          413,  414,  419,  421,  449,  450,  454,  468,  469,  476,  499,  500,\n",
       "          503,  505,  506,  521,  528,  530,  539,  556,  560,  570,  606,  617,\n",
       "          631,  635,  639,  644,  655,  661,  672,  682,  684,  702,  737,  743,\n",
       "          748,  756,  759,  762,  770,  784,  798,  814,  826,  831,  835,  858,\n",
       "          868,  884,  908,  925,  931,  946,  964,  966,  979,  980,  987, 1011,\n",
       "         1032, 1058, 1076, 1101, 1139, 1140, 1155, 1160, 1163, 1165, 1171, 1205,\n",
       "         1211, 1225, 1251, 1272, 1281, 1283, 1286, 1304, 1312, 1357, 1362, 1390,\n",
       "         1399, 1410, 1420, 1425, 1439, 1469, 1479, 1490, 1491, 1543, 1549, 1574,\n",
       "         1591, 1604, 1608, 1612, 1614, 1628, 1647, 1652, 1659, 1660, 1700, 1727,\n",
       "         1732, 1741, 1745, 1768, 1799, 1802, 1821, 1824, 1834, 1845, 1849, 1860,\n",
       "         1863, 1884, 1898, 1929, 1948, 1951, 1968, 1971, 2006, 2014, 2026, 2047,\n",
       "         2075, 2076, 2123, 2129, 2137, 2145, 2164, 2181, 2195, 2196, 2220, 2224,\n",
       "         2229, 2236, 2245, 2248, 2255, 2262, 2279, 2285, 2290, 2295, 2317, 2320,\n",
       "         2337, 2341, 2369, 2378, 2398, 2413, 2417, 2419, 2422, 2425, 2427, 2436,\n",
       "         2445, 2449, 2469, 2475, 2479, 2489, 2492, 2508, 2515, 2532, 2543, 2549,\n",
       "         2597, 2604, 2609, 2613, 2626, 2629, 2631, 2657, 2666, 2674, 2675, 2686,\n",
       "         2692, 2702, 2707, 2708, 2717, 2719, 2722, 2743, 2765, 2772, 2780, 2793,\n",
       "         2798, 2812, 2817, 2849]),\n",
       " 'seen_ids': tensor([[  0,   0,   0,  ..., 316, 677, 768],\n",
       "         [  0,   0,   0,  ..., 269, 696, 768],\n",
       "         [  0,   0,   0,  ..., 345, 520, 768],\n",
       "         ...,\n",
       "         [  0,   0,   0,  ..., 313, 669, 768],\n",
       "         [  0,   0,   0,  ..., 346, 566, 768],\n",
       "         [  0,   0,   0,  ..., 295, 644, 768]]),\n",
       " 'target': tensor([[ 43, 366, 756, 768],\n",
       "         [135, 386, 589, 768],\n",
       "         [104, 260, 652, 768],\n",
       "         ...,\n",
       "         [ 37, 470, 715, 769],\n",
       "         [187, 316, 567, 768],\n",
       "         [173, 503, 609, 769]]),\n",
       " 'attention_mask': tensor([[0., 0., 0.,  ..., 1., 1., 1.],\n",
       "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
       "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
       "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
       "         [0., 0., 0.,  ..., 1., 1., 1.]])}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_val = next(iter(eval_loader))\n",
    "batch_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.894140625\n"
     ]
    },
    {
     "ename": "MisconfigurationException",
     "evalue": "You are trying to `self.log()` but the loop's result collection is not registered yet. This is most likely because you are trying to log in a `predict` hook, but it doesn't support logging",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mseqrec_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gusak/semantic_seqrec/notebooks/../src/modules.py:208\u001b[0m, in \u001b[0;36mSeqRecHuggingface.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    205\u001b[0m     targets \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minv_map\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mtuple\u001b[39m(seq), \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[1;32m    206\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_val_metrics(targets, preds)\n\u001b[0;32m--> 208\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval_ndcg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mndcg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprog_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_hit_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhit_rate\u001b[39m\u001b[38;5;124m'\u001b[39m], prog_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_mrr\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmrr\u001b[39m\u001b[38;5;124m'\u001b[39m], prog_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.mlspace/envs/splits/lib/python3.10/site-packages/pytorch_lightning/core/module.py:449\u001b[0m, in \u001b[0;36mLightningModule.log\u001b[0;34m(self, name, value, prog_bar, logger, on_step, on_epoch, reduce_fx, enable_graph, sync_dist, sync_dist_group, add_dataloader_idx, batch_size, metric_attribute, rank_zero_only)\u001b[0m\n\u001b[1;32m    447\u001b[0m results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39m_results\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\n\u001b[1;32m    450\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to `self.log()` but the loop\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms result collection is not registered\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m yet. This is most likely because you are trying to log in a `predict` hook,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt support logging\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m     )\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\n\u001b[1;32m    456\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to `self.log()` but it is not managed by the `Trainer` control flow\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    457\u001b[0m     )\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: You are trying to `self.log()` but the loop's result collection is not registered yet. This is most likely because you are trying to log in a `predict` hook, but it doesn't support logging"
     ]
    }
   ],
   "source": [
    "seqrec_module.validation_step(batch_val, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ndcg': 0.0, 'hit_rate': 0.0, 'mrr': 0.0}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = batch_val['target'].detach().cpu().numpy()\n",
    "targets = [seqrec_module.inv_map.get(tuple(seq), 0) for seq in targets]\n",
    "targets\n",
    "metrics = seqrec_module.compute_val_metrics(targets, preds)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6172"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_map[(110, 393, 628, 768)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110, 393, 628, 768)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(batch_val['target'][0].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mseqrec_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_prediction_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_val\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gusak/semantic_seqrec/notebooks/../src/modules.py:254\u001b[0m, in \u001b[0;36mSeqRecHuggingface.make_prediction_generate\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    250\u001b[0m     cont \u001b[38;5;241m=\u001b[39m seq\u001b[38;5;241m.\u001b[39msequences[:, \u001b[38;5;241m-\u001b[39mN:] \u001b[38;5;66;03m# (B*K, N)\u001b[39;00m\n\u001b[1;32m    252\u001b[0m cont \u001b[38;5;241m=\u001b[39m cont\u001b[38;5;241m.\u001b[39mview(B, K, N) \u001b[38;5;66;03m#  -> (B, K, N)\u001b[39;00m\n\u001b[0;32m--> 254\u001b[0m cont_flat \u001b[38;5;241m=\u001b[39m \u001b[43mcont\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    255\u001b[0m preds_flat \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minv_map\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mtuple\u001b[39m(seq), \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m cont_flat]\n\u001b[1;32m    256\u001b[0m preds \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mtensor(preds_flat, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mview(B, K))  \u001b[38;5;66;03m#  -> (B, K)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "seqrec_module.make_prediction_generate(batch_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqrec_module.mode = 'beamsearch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = batch_val['input_ids'].size(0)\n",
    "K = seqrec_module.predict_top_k\n",
    "N = seqrec_module.N\n",
    "\n",
    "if seqrec_module.mode == 'greedy':\n",
    "    max_new_tokens = N * K\n",
    "    # гриди: do_sample=False, num_beams=None, num_return_sequences=1\n",
    "    params = dict(seqrec_module.generate_params)\n",
    "    params[\"do_sample\"] = False\n",
    "    params[\"max_new_tokens\"] = max_new_tokens\n",
    "\n",
    "    seq = seqrec_module.model.generate(\n",
    "        batch_val['input_ids'][:, -seqrec_module.model.config.n_positions + max_new_tokens:].to(seqrec_module.model.device),\n",
    "        pad_token_id=seqrec_module.padding_idx,\n",
    "        **params\n",
    "    )\n",
    "\n",
    "    cont = seq[:, -max_new_tokens:].view(B, K, N)  # (B, N*K)\n",
    "\n",
    "elif seqrec_module.mode == 'beamsearch':\n",
    "    params = dict(seqrec_module.generate_params)\n",
    "    params[\"do_sample\"] = False\n",
    "    params[\"num_beams\"] = K\n",
    "    params[\"num_return_sequences\"] = K\n",
    "    params[\"max_new_tokens\"] = N\n",
    "    seq = seqrec_module.model.generate(\n",
    "        batch_val['input_ids'][:, -seqrec_module.model.config.n_positions + N:].to(seqrec_module.model.device),\n",
    "        pad_token_id=seqrec_module.padding_idx,\n",
    "        **params\n",
    "    )\n",
    "    print('jopa')\n",
    "\n",
    "    cont = seq[:, -N:] # (B*K, N)\n",
    "\n",
    "cont = cont.view(B, K, N) #  -> (B, K, N)\n",
    "\n",
    "cont_flat = cont.reshape(-1, N).tolist()\n",
    "preds_flat = [seqrec_module.inv_map.get(tuple(seq), 0) for seq in cont_flat]\n",
    "preds = (torch.tensor(preds_flat, dtype=torch.long).view(B, K))  #  -> (B, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 10, 4])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    0,    0,  ...,    0,    0,    0],\n",
       "        [9289, 9289, 9289,  ..., 9289, 9289, 9289],\n",
       "        [9289, 9289, 9289,  ..., 9289, 9289, 9289],\n",
       "        ...,\n",
       "        [9289, 9289, 9289,  ..., 9289, 9289, 9289],\n",
       "        [9289, 9289, 9289,  ..., 9289, 9289, 9289],\n",
       "        [9289, 9289, 9289,  ..., 9289, 9289, 9289]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[9, 8, 7,  ..., 2, 1, 0],\n",
       "        [9, 8, 7,  ..., 2, 1, 0],\n",
       "        [9, 8, 7,  ..., 2, 1, 0],\n",
       "        ...,\n",
       "        [9, 8, 7,  ..., 2, 1, 0],\n",
       "        [9, 8, 7,  ..., 2, 1, 0],\n",
       "        [9, 8, 7,  ..., 2, 1, 0]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = preds != 0\n",
    "print(mask.sum().item() / mask.numel())\n",
    "\n",
    "scores = torch.arange(K-1, -1, -1)   # [K-1, ... 0]\n",
    "scores = scores.unsqueeze(0).expand(B, -1)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 768,  768,  768,  768],\n",
       "         [ 768,  768,  768,  768],\n",
       "         [ 768,  768,  768,  768],\n",
       "         ...,\n",
       "         [ 768,  768,  768,  768],\n",
       "         [ 768,  768,  768, 1462],\n",
       "         [1462, 1462, 1462, 1462]],\n",
       "\n",
       "        [[ 768,  768,  768,  768],\n",
       "         [ 768,  768,  768,  768],\n",
       "         [ 768,  768,  768,  768],\n",
       "         ...,\n",
       "         [ 347,  347,  347,  347],\n",
       "         [ 347,  347,  347,  347],\n",
       "         [ 347, 1324, 1324, 1324]],\n",
       "\n",
       "        [[ 768,  768,  768,  768],\n",
       "         [ 768,  768,  768,  768],\n",
       "         [ 768,  768,  768,  768],\n",
       "         ...,\n",
       "         [ 768,  768,  768,  768],\n",
       "         [ 768,  768, 1469, 1469],\n",
       "         [1469, 1469, 1469, 1469]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1658, 1658, 1658, 1658],\n",
       "         [1658, 1658, 1658, 1658],\n",
       "         [1658, 1658, 1658, 1658],\n",
       "         ...,\n",
       "         [1023, 1023, 1023, 1023],\n",
       "         [1023, 1023, 1023, 1023],\n",
       "         [1023, 1023, 1023, 1023]],\n",
       "\n",
       "        [[ 768,  768,  768,  768],\n",
       "         [ 768,  768,  768,  768],\n",
       "         [ 768,  768,  768,  768],\n",
       "         ...,\n",
       "         [ 768,  768,  768,  768],\n",
       "         [ 768,  768,  768,  768],\n",
       "         [1357, 1357, 1357, 1357]],\n",
       "\n",
       "        [[ 768,  768,  768,  768],\n",
       "         [ 768,  768,  768,  768],\n",
       "         [ 768,  768,  768,  768],\n",
       "         ...,\n",
       "         [ 768,  768,  768,  768],\n",
       "         [ 768, 1224, 1224, 1224],\n",
       "         [1224, 1224, 1224, 1224]]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 768,  768,  768,  768],\n",
       "        [ 768,  768,  768,  768],\n",
       "        [ 768,  768,  768,  768],\n",
       "        ...,\n",
       "        [ 768,  768,  768,  768],\n",
       "        [ 768, 1224, 1224, 1224],\n",
       "        [1224, 1224, 1224, 1224]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont.reshape(-1, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[110, 393, 628, 768],\n",
       "        [ 23, 293, 548, 768],\n",
       "        [ 43, 366, 756, 768],\n",
       "        ...,\n",
       "        [120, 387, 567, 768],\n",
       "        [193, 390, 621, 768],\n",
       "        [ 83, 302, 716, 769]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_val['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[110, 393, 628, 768],\n",
       "        [ 53, 261, 585, 768],\n",
       "        [135, 386, 589, 768],\n",
       "        ...,\n",
       "        [126, 378, 609, 768],\n",
       "        [240, 360, 765, 768],\n",
       "        [104, 450, 701, 768]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1707"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqrec_module.model.config.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.mlspace/envs/splits/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:653: Checkpoint directory /home/jovyan/gusak/semantic_seqrec/models/global_timesplit/val_by_time/Beauty/q09/GPT-2 exists and is not empty.\n",
      "\n",
      "  | Name  | Type            | Params\n",
      "------------------------------------------\n",
      "0 | model | GPT2LMHeadModel | 242 K \n",
      "------------------------------------------\n",
      "242 K     Trainable params\n",
      "0         Non-trainable params\n",
      "242 K     Total params\n",
      "0.968     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "Epoch 0:   0%|          | 0/159 [00:00<?, ?it/s]                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 159/159 [01:05<00:00,  2.42it/s, v_num=6]0.894140625\n",
      "0.879296875\n",
      "0.903125\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "a Tensor with 10 elements cannot be converted to Scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseqrec_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.mlspace/envs/splits/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.mlspace/envs/splits/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/.mlspace/envs/splits/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.mlspace/envs/splits/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:987\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 987\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    992\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.mlspace/envs/splits/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1033\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1033\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.mlspace/envs/splits/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.mlspace/envs/splits/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.mlspace/envs/splits/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:141\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(data_fetcher)\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_advance_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/.mlspace/envs/splits/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:295\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.on_advance_end\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_accumulate():\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;66;03m# clear gradients to not leave any unused memory during validation\u001b[39;00m\n\u001b[1;32m    293\u001b[0m     call\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_validation_model_zero_grad\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 295\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39m_first_loop_iter \u001b[38;5;241m=\u001b[39m first_loop_iter\n",
      "File \u001b[0;32m~/.mlspace/envs/splits/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:182\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.mlspace/envs/splits/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:135\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.mlspace/envs/splits/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:396\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    390\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    395\u001b[0m )\n\u001b[0;32m--> 396\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/.mlspace/envs/splits/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    312\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/.mlspace/envs/splits/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:412\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gusak/semantic_seqrec/notebooks/../src/modules.py:206\u001b[0m, in \u001b[0;36mSeqRecHuggingface.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    204\u001b[0m     targets \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    205\u001b[0m     targets \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minv_map\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mtuple\u001b[39m(seq), \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[0;32m--> 206\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_val_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_ndcg\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mndcg\u001b[39m\u001b[38;5;124m'\u001b[39m], prog_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_hit_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhit_rate\u001b[39m\u001b[38;5;124m'\u001b[39m], prog_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/gusak/semantic_seqrec/notebooks/../src/modules.py:84\u001b[0m, in \u001b[0;36mSeqRecBase.compute_val_metrics\u001b[0;34m(self, targets, preds)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misin(targets[i], pred)\u001b[38;5;241m.\u001b[39mitem():\n\u001b[1;32m     83\u001b[0m     hit_rate \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 84\u001b[0m     rank \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     85\u001b[0m     ndcg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mlog2(rank \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     86\u001b[0m     mrr \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m rank\n",
      "\u001b[0;31mRuntimeError\u001b[0m: a Tensor with 10 elements cannot be converted to Scalar"
     ]
    }
   ],
   "source": [
    "trainer.fit(model=seqrec_module,\n",
    "                    train_dataloaders=train_loader,\n",
    "                    val_dataloaders=eval_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.model.model_class == 'GPT-2':\n",
    "    if config.model.generation:\n",
    "        predict_dataset = CausalLMPredictionDataset(\n",
    "            test, max_length=config.dataset_params.max_length - max(config.evaluator.top_k))\n",
    "        \n",
    "        predict_loader = DataLoader(\n",
    "                predict_dataset, shuffle=False,\n",
    "                collate_fn=PaddingCollateFn(left_padding=True),\n",
    "                batch_size=config.dataloader.test_batch_size,\n",
    "                num_workers=config.dataloader.num_workers)\n",
    "        \n",
    "        seqrec_module.set_predict_mode(generate=True, mode=config.model.mode, **config.model.generation_params)\n",
    "\n",
    "    else:\n",
    "        predict_dataset = CausalLMPredictionDataset(test, max_length=config.dataset_params.max_length)\n",
    "\n",
    "        predict_loader = DataLoader(\n",
    "                predict_dataset, shuffle=False,\n",
    "                collate_fn=PaddingCollateFn(),\n",
    "                batch_size=config.dataloader.test_batch_size,\n",
    "                num_workers=config.dataloader.num_workers)\n",
    "        \n",
    "        seqrec_module.set_predict_mode(generate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 96/96 [00:01<00:00, 48.90it/s]\n"
     ]
    }
   ],
   "source": [
    "preds = trainer.predict(model=seqrec_module, dataloaders=predict_loader)\n",
    "recs = preds2recs(preds, successive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10148</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>10163</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>7849</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>9509</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>10609</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61095</th>\n",
       "      <td>22361</td>\n",
       "      <td>7906</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61096</th>\n",
       "      <td>22361</td>\n",
       "      <td>9062</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61097</th>\n",
       "      <td>22361</td>\n",
       "      <td>4973</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61098</th>\n",
       "      <td>22361</td>\n",
       "      <td>5689</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61099</th>\n",
       "      <td>22361</td>\n",
       "      <td>9561</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id  item_id  prediction\n",
       "0            0    10148    1.000000\n",
       "1            0    10163    0.500000\n",
       "2            0     7849    0.333333\n",
       "3            0     9509    0.250000\n",
       "4            0    10609    0.200000\n",
       "...        ...      ...         ...\n",
       "61095    22361     7906    0.166667\n",
       "61096    22361     9062    0.142857\n",
       "61097    22361     4973    0.125000\n",
       "61098    22361     5689    0.111111\n",
       "61099    22361     9561    0.100000\n",
       "\n",
       "[61100 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(predict_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'early_stopping': False,\n",
       " 'num_return_sequences': 1,\n",
       " 'do_sample': False,\n",
       " 'no_repeat_ngram_size': 1,\n",
       " 'num_beams': 10}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqrec_module.generate_params['num_return_sequences'] = 1\n",
    "seqrec_module.generate_params['no_repeat_ngram_size'] = 1\n",
    "seqrec_module.generate_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFace Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = seqrec_module.model.generate(\n",
    "                batch['input_ids'][:, -seqrec_module.model.config.n_positions + seqrec_module.predict_top_k*4:].to(seqrec_module.model.device),\n",
    "                pad_token_id=seqrec_module.padding_idx,\n",
    "                max_new_tokens=seqrec_module.predict_top_k*4,\n",
    "                **seqrec_module.generate_params,\n",
    "            )\n",
    "\n",
    "preds = seq[:, -seqrec_module.predict_top_k*4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 68])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 28])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 68])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 40])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(seq, size):\n",
    "    return [seq[pos:pos + size] for pos in range(0, len(seq), size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[10148, 10163, 10632,  3496],\n",
       "         [ 7532,  8124,  8824,  8969],\n",
       "         [10100, 10106,  1880,  5636],\n",
       "         ...,\n",
       "         [ 5631,  5633,  5634,  5638],\n",
       "         [ 8601,  8635,  8636,  8701],\n",
       "         [10206, 10208, 10244, 10247]],\n",
       "\n",
       "        [[ 1390,  1660,  3519,  1679],\n",
       "         [ 5516,  5695,  9568, 10646],\n",
       "         [11540, 11517, 11630, 10442],\n",
       "         ...,\n",
       "         [10123, 10206, 10208, 10244],\n",
       "         [10247, 10285,  5637,  6288],\n",
       "         [11932, 11665, 11315, 11609]],\n",
       "\n",
       "        [[10350,  8507,  8968,  9767],\n",
       "         [10632,  6237,  7532,  8136],\n",
       "         [ 8969,  8824, 10000, 10100],\n",
       "         ...,\n",
       "         [ 9205, 10159, 10189, 10770],\n",
       "         [10313, 11215, 10619, 10166],\n",
       "         [10746, 10248, 10354, 11141]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[11290,  1679,  6136, 11690],\n",
       "         [11691, 11790, 11932,  5752],\n",
       "         [ 5921,  6293,  6371,  6661],\n",
       "         ...,\n",
       "         [11844, 11770, 11766, 11775],\n",
       "         [11774, 11715, 11763, 11778],\n",
       "         [11765, 11773,  8979, 11803]],\n",
       "\n",
       "        [[ 6046,  8662,  6650, 10991],\n",
       "         [ 8664,  8665,  9236, 11146],\n",
       "         [11160, 10895, 10898, 10899],\n",
       "         ...,\n",
       "         [10123, 10206, 10208, 10243],\n",
       "         [10244, 10247, 10285,  5637],\n",
       "         [ 8108,  8518,  8596,  8705]],\n",
       "\n",
       "        [[10569, 10697, 10695, 10700],\n",
       "         [10538, 10895, 10898, 10899],\n",
       "         [10905, 11374, 11499, 11220],\n",
       "         ...,\n",
       "         [11515, 11766, 11802, 11800],\n",
       "         [11776,  8979, 11715, 11763],\n",
       "         [11714, 11699, 11803, 11777]]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.view(64,-1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10148, 10163, 10632,  3496,  7532,  8124,  8824,  8969, 10100, 10106,\n",
       "         1880,  5636,  5647,  5752,  5921,  6293,  6371,  6661,  7608,  8092,\n",
       "         8518,  8596,  8705,  8858,  8897,  8744,  4858,  5266,  5631,  5633,\n",
       "         5634,  5638,  8601,  8635,  8636,  8701, 10206, 10208, 10244, 10247])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7977,  9509,  7849,  ...,  9120,  9622, 10000],\n",
       "        [10214,  4343,  6727,  ...,  7847,  5764,  4185],\n",
       "        [10632,  6237,  8136,  ..., 10156,  9205,  9110],\n",
       "        ...,\n",
       "        [ 6046,  3578,  9668,  ..., 11949, 11972, 11770],\n",
       "        [ 2648,  5095,  2319,  ...,  7650,  7652, 10284],\n",
       "        [ 8229, 11054,  8978,  ..., 11699, 11803, 11236]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'early_stopping': False,\n",
       " 'num_return_sequences': 10,\n",
       " 'do_sample': False,\n",
       " 'no_repeat_ngram_size': 1,\n",
       " 'num_beams': 10}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqrec_module.generate_params['num_return_sequences'] = seqrec_module.predict_top_k\n",
    "seqrec_module.generate_params['no_repeat_ngram_size'] = 1\n",
    "seqrec_module.generate_params['num_beams'] = seqrec_module.predict_top_k\n",
    "seqrec_module.generate_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = seqrec_module.model.generate(\n",
    "                batch['input_ids'][:, -seqrec_module.model.config.n_positions + 4:].to(seqrec_module.model.device),\n",
    "                pad_token_id=seqrec_module.padding_idx,\n",
    "                max_new_tokens=4,\n",
    "                **seqrec_module.generate_params,\n",
    "            )\n",
    "\n",
    "preds = seq[:, -4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([640, 4])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([640, 32])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10148, 10163,  7849,  7977],\n",
       "        [10148, 10163,  7849,  9509],\n",
       "        [10148, 10163,  7849, 10003],\n",
       "        ...,\n",
       "        [ 2618,  2620,  2617,  9250],\n",
       "        [ 2618,  2619,  4439,  9250],\n",
       "        [ 2618,  4439,  2617,  9250]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[10148, 10163,  7849,  7977],\n",
       "         [10148, 10163,  7849,  9509],\n",
       "         [10148, 10163,  7849, 10003],\n",
       "         ...,\n",
       "         [10148, 10163, 10632,  6237],\n",
       "         [10148, 10163,  7532,  8824],\n",
       "         [10148, 10163, 10632,  8136]],\n",
       "\n",
       "        [[ 2648,  3145, 11940, 11941],\n",
       "         [ 2648,  3145, 11938, 11942],\n",
       "         [ 2648,  3145, 11938, 11940],\n",
       "         ...,\n",
       "         [ 2648,  2673,  2826,  7987],\n",
       "         [ 5040,  7637,  6914,  8326],\n",
       "         [ 9165, 10481,  9259,  9084]],\n",
       "\n",
       "        [[11215,  3496,  7532,  8824],\n",
       "         [11215,  3496,  6237,  9109],\n",
       "         [11215,  3496,  6237,  7532],\n",
       "         ...,\n",
       "         [10100, 10163,  6237,  7532],\n",
       "         [11215,  3496,  7532,  8124],\n",
       "         [11215,  3496,  6237,  8968]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 5752,  5921,  6293,  6371],\n",
       "         [ 5752,  5921,  6293,  6661],\n",
       "         [10742,  8139, 10345, 11408],\n",
       "         ...,\n",
       "         [ 5752,  5921,  6293,  5131],\n",
       "         [ 5752,  5921,  6293, 10350],\n",
       "         [10742,  8139,  6136, 11691]],\n",
       "\n",
       "        [[ 9668,  4037,  4858,  5244],\n",
       "         [ 3488,  4858,  4037,  7130],\n",
       "         [ 9668,  4037,  5528,  7122],\n",
       "         ...,\n",
       "         [ 9668,  4037,  4858,  5944],\n",
       "         [ 9668,  4037,  4858,  5528],\n",
       "         [ 9668,  2605,  5677,  6510]],\n",
       "\n",
       "        [[ 2618,  2619,  2620,  4439],\n",
       "         [ 2618,  2620,  2617,  4439],\n",
       "         [ 2618,  2620,  2617,  2619],\n",
       "         ...,\n",
       "         [ 2618,  2620,  2617,  9250],\n",
       "         [ 2618,  2619,  4439,  9250],\n",
       "         [ 2618,  4439,  2617,  9250]]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.view(64, -1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
       "        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = torch.arange(10-1, -1, -1)   # [K-1, ... 0]\n",
    "scores = scores.unsqueeze(0).expand(64, -1)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{9450: array([104, 431, 695, 768]),\n",
       " 9840: array([211, 332, 621, 768]),\n",
       " 10077: array([193, 307, 697, 768]),\n",
       " 11156: array([ 84, 430, 657, 768]),\n",
       " 11753: array([193, 436, 645, 768]),\n",
       " 11864: array([  1, 295, 720, 768]),\n",
       " 3310: array([120, 291, 761, 768]),\n",
       " 4573: array([235, 506, 751, 768]),\n",
       " 4137: array([177, 323, 750, 768]),\n",
       " 9080: array([177, 323, 677, 768]),\n",
       " 10956: array([140, 396, 585, 768]),\n",
       " 4387: array([ 15, 386, 559, 768]),\n",
       " 6363: array([ 15, 269, 696, 768]),\n",
       " 454: array([254, 289, 674, 768]),\n",
       " 4209: array([ 92, 463, 696, 768]),\n",
       " 59: array([107, 300, 715, 768]),\n",
       " 5666: array([211, 470, 675, 768]),\n",
       " 8650: array([  2, 434, 724, 768]),\n",
       " 9405: array([  6, 377, 571, 768]),\n",
       " 8877: array([  1, 378, 609, 768]),\n",
       " 8824: array([ 37, 314, 743, 768]),\n",
       " 8651: array([ 37, 374, 573, 768]),\n",
       " 10131: array([ 75, 378, 751, 768]),\n",
       " 10254: array([  1, 508, 688, 768]),\n",
       " 8969: array([  1, 378, 548, 768]),\n",
       " 5098: array([211, 378, 751, 768]),\n",
       " 2714: array([ 74, 348, 644, 768]),\n",
       " 10321: array([  2, 408, 512, 768]),\n",
       " 10163: array([ 60, 290, 751, 768]),\n",
       " 8072: array([  2, 500, 644, 768]),\n",
       " 9767: array([200, 427, 645, 768]),\n",
       " 3496: array([200, 365, 761, 768]),\n",
       " 7450: array([ 60, 277, 697, 768]),\n",
       " 8030: array([ 74, 296, 675, 768]),\n",
       " 9622: array([211, 378, 720, 768]),\n",
       " 10228: array([126, 388, 609, 768]),\n",
       " 8936: array([  2, 388, 750, 768]),\n",
       " 6005: array([  2, 378, 720, 768]),\n",
       " 3615: array([  2, 489, 716, 768]),\n",
       " 9212: array([ 60, 424, 621, 768]),\n",
       " 4644: array([ 60, 408, 697, 768]),\n",
       " 8662: array([  2, 431, 720, 768]),\n",
       " 3906: array([211, 296, 522, 768]),\n",
       " 9581: array([ 60, 378, 609, 768]),\n",
       " 10148: array([200, 424, 761, 768]),\n",
       " 8347: array([  2, 313, 715, 768]),\n",
       " 4277: array([  6, 377, 571, 769]),\n",
       " 9857: array([ 32, 470, 628, 768]),\n",
       " 2939: array([177, 378, 674, 768]),\n",
       " 10926: array([177, 378, 641, 768]),\n",
       " 9885: array([211, 470, 522, 768]),\n",
       " 3768: array([ 83, 289, 585, 768]),\n",
       " 3082: array([ 83, 289, 585, 769]),\n",
       " 599: array([  6, 377, 571, 770]),\n",
       " 4185: array([  6, 377, 571, 771]),\n",
       " 2420: array([  6, 377, 571, 772]),\n",
       " 9201: array([ 60, 378, 720, 768]),\n",
       " 1178: array([ 53, 470, 559, 768]),\n",
       " 948: array([ 53, 302, 573, 768]),\n",
       " 457: array([177, 423, 670, 768]),\n",
       " 2922: array([ 44, 378, 670, 768]),\n",
       " 5381: array([  6, 377, 571, 773]),\n",
       " 3000: array([  2, 470, 574, 768]),\n",
       " 3905: array([  2, 289, 559, 768]),\n",
       " 7560: array([128, 400, 721, 768]),\n",
       " 3904: array([187, 312, 745, 768]),\n",
       " 7140: array([ 44, 431, 720, 768]),\n",
       " 7857: array([158, 394, 605, 768]),\n",
       " 6607: array([208, 354, 571, 768]),\n",
       " 9560: array([  6, 377, 571, 774]),\n",
       " 8608: array([  6, 377, 571, 775]),\n",
       " 10266: array([  6, 377, 571, 776]),\n",
       " 1521: array([  6, 377, 571, 777]),\n",
       " 3896: array([220, 296, 720, 768]),\n",
       " 6606: array([  1, 478, 719, 768]),\n",
       " 7849: array([128, 510, 655, 768]),\n",
       " 8613: array([ 15, 386, 670, 768]),\n",
       " 7397: array([  6, 377, 571, 778]),\n",
       " 10042: array([ 92, 463, 696, 769]),\n",
       " 7789: array([  2, 394, 697, 768]),\n",
       " 7862: array([236, 479, 762, 768]),\n",
       " 5181: array([ 13, 299, 677, 768]),\n",
       " 8482: array([228, 345, 718, 768]),\n",
       " 5850: array([211, 378, 695, 768]),\n",
       " 10322: array([ 54, 405, 560, 768]),\n",
       " 8466: array([ 92, 354, 746, 768]),\n",
       " 10364: array([ 15, 307, 548, 768]),\n",
       " 7524: array([  1, 500, 695, 768]),\n",
       " 7922: array([ 54, 377, 561, 768]),\n",
       " 3094: array([211, 312, 559, 768]),\n",
       " 9494: array([126, 503, 560, 768]),\n",
       " 10842: array([208, 402, 521, 768]),\n",
       " 7042: array([ 44, 378, 751, 768]),\n",
       " 1219: array([211, 503, 573, 768]),\n",
       " 455: array([ 28, 354, 621, 768]),\n",
       " 10113: array([211, 470, 745, 768]),\n",
       " 278: array([211, 459, 745, 768]),\n",
       " 10094: array([  1, 277, 721, 768]),\n",
       " 3227: array([239, 419, 746, 768]),\n",
       " 8043: array([ 95, 378, 719, 768]),\n",
       " 10574: array([211, 470, 560, 768]),\n",
       " 3531: array([  2, 277, 721, 768]),\n",
       " 5024: array([  2, 305, 539, 768]),\n",
       " 4196: array([219, 316, 560, 768]),\n",
       " 6969: array([177, 302, 715, 768]),\n",
       " 4235: array([ 44, 316, 629, 768]),\n",
       " 5063: array([ 83, 470, 715, 768]),\n",
       " 4195: array([ 74, 478, 766, 768]),\n",
       " 10580: array([  2, 348, 719, 768]),\n",
       " 7298: array([158, 378, 573, 768]),\n",
       " 11349: array([ 15, 286, 685, 768]),\n",
       " 11234: array([ 15, 378, 720, 768]),\n",
       " 1190: array([  1, 371, 567, 768]),\n",
       " 8748: array([126, 409, 751, 768]),\n",
       " 3313: array([ 28, 459, 628, 768]),\n",
       " 6231: array([  2, 388, 662, 768]),\n",
       " 11793: array([ 83, 470, 559, 768]),\n",
       " 5095: array([219, 462, 766, 768]),\n",
       " 5527: array([ 60, 302, 628, 768]),\n",
       " 187: array([177, 318, 674, 768]),\n",
       " 3514: array([177, 318, 721, 768]),\n",
       " 10287: array([221, 345, 580, 768]),\n",
       " 8199: array([138, 476, 652, 768]),\n",
       " 1675: array([120, 410, 751, 768]),\n",
       " 664: array([  2, 272, 512, 768]),\n",
       " 2244: array([ 32, 371, 628, 768]),\n",
       " 2646: array([140, 495, 751, 768]),\n",
       " 7224: array([  6, 377, 571, 779]),\n",
       " 8844: array([ 28, 503, 655, 768]),\n",
       " 11361: array([ 60, 348, 559, 768]),\n",
       " 13: array([ 28, 378, 697, 768]),\n",
       " 6281: array([211, 503, 522, 768]),\n",
       " 9432: array([211, 272, 701, 768]),\n",
       " 7122: array([  2, 459, 609, 768]),\n",
       " 10035: array([138, 277, 745, 768]),\n",
       " 10609: array([211, 503, 560, 768]),\n",
       " 10230: array([211, 503, 685, 768]),\n",
       " 9194: array([ 28, 348, 701, 768]),\n",
       " 8446: array([211, 377, 580, 768]),\n",
       " 9195: array([211, 503, 560, 769]),\n",
       " 10258: array([177, 309, 696, 768]),\n",
       " 10347: array([208, 440, 721, 768]),\n",
       " 2162: array([  6, 377, 571, 780]),\n",
       " 82: array([  1, 503, 745, 768]),\n",
       " 7360: array([ 15, 400, 623, 768]),\n",
       " 7756: array([ 92, 354, 685, 768]),\n",
       " 6000: array([158, 503, 641, 768]),\n",
       " 7295: array([  2, 503, 751, 768]),\n",
       " 6309: array([ 32, 278, 765, 768]),\n",
       " 437: array([233, 413, 559, 768]),\n",
       " 7656: array([140, 260, 566, 768]),\n",
       " 6789: array([150, 316, 585, 768]),\n",
       " 4505: array([211, 290, 539, 768]),\n",
       " 2100: array([ 95, 476, 701, 768]),\n",
       " 4748: array([ 62, 510, 559, 768]),\n",
       " 4958: array([ 22, 272, 716, 768]),\n",
       " 331: array([ 44, 274, 534, 768]),\n",
       " 9417: array([138, 510, 644, 768]),\n",
       " 332: array([211, 348, 720, 768]),\n",
       " 2035: array([  2, 459, 559, 768]),\n",
       " 2017: array([  2, 470, 721, 768]),\n",
       " 3474: array([138, 390, 718, 768]),\n",
       " 4061: array([138, 269, 583, 768]),\n",
       " 8826: array([ 80, 479, 664, 768]),\n",
       " 7229: array([ 22, 345, 686, 768]),\n",
       " 10762: array([ 54, 407, 571, 768]),\n",
       " 10798: array([ 83, 371, 559, 768]),\n",
       " 1416: array([  6, 377, 571, 781]),\n",
       " 5015: array([  1, 371, 751, 768]),\n",
       " 5016: array([  1, 371, 567, 769]),\n",
       " 7772: array([ 32, 348, 745, 768]),\n",
       " 9914: array([121, 309, 621, 768]),\n",
       " 5605: array([ 80, 479, 664, 769]),\n",
       " 5145: array([ 37, 312, 560, 768]),\n",
       " 6050: array([126, 503, 685, 768]),\n",
       " 9778: array([169, 479, 646, 768]),\n",
       " 11149: array([ 11, 278, 628, 768]),\n",
       " 15: array([211, 459, 609, 768]),\n",
       " 9562: array([211, 459, 609, 769]),\n",
       " 5490: array([177, 290, 715, 768]),\n",
       " 5523: array([177, 290, 621, 768]),\n",
       " 8880: array([ 28, 394, 746, 768]),\n",
       " 11453: array([ 13, 261, 696, 768]),\n",
       " 271: array([  6, 377, 571, 782]),\n",
       " 10085: array([ 15, 377, 539, 768]),\n",
       " 4668: array([ 15, 400, 697, 768]),\n",
       " 277: array([ 15, 460, 539, 768]),\n",
       " 8684: array([  2, 470, 571, 768]),\n",
       " 9369: array([224, 346, 571, 768]),\n",
       " 10598: array([  6, 377, 571, 783]),\n",
       " 3232: array([  6, 377, 571, 784]),\n",
       " 8583: array([ 83, 378, 580, 768]),\n",
       " 10856: array([ 84, 400, 715, 768]),\n",
       " 6727: array([ 28, 313, 715, 768]),\n",
       " 8569: array([140, 324, 641, 768]),\n",
       " 10996: array([160, 421, 547, 768]),\n",
       " 10511: array([138, 300, 718, 768]),\n",
       " 11138: array([ 37, 360, 560, 768]),\n",
       " 5040: array([ 75, 290, 718, 768]),\n",
       " 1824: array([183, 372, 571, 768]),\n",
       " 8990: array([ 27, 340, 640, 768]),\n",
       " 4733: array([177, 470, 621, 768]),\n",
       " 3404: array([211, 348, 720, 769]),\n",
       " 10003: array([ 44, 348, 753, 768]),\n",
       " 10942: array([177, 460, 751, 768]),\n",
       " 6903: array([219, 402, 609, 768]),\n",
       " 6993: array([ 95, 503, 628, 768]),\n",
       " 8471: array([211, 470, 564, 768]),\n",
       " 6944: array([  6, 377, 571, 785]),\n",
       " 2847: array([140, 396, 646, 768]),\n",
       " 6251: array([ 33, 280, 734, 768]),\n",
       " 688: array([ 33, 392, 743, 768]),\n",
       " 7117: array([ 95, 377, 745, 768]),\n",
       " 9031: array([233, 413, 628, 768]),\n",
       " 10545: array([126, 378, 745, 768]),\n",
       " 9947: array([126, 394, 658, 768]),\n",
       " 185: array([ 44, 316, 559, 768]),\n",
       " 4563: array([  2, 348, 745, 768]),\n",
       " 8122: array([  2, 408, 628, 768]),\n",
       " 9229: array([  1, 500, 675, 768]),\n",
       " 1880: array([211, 377, 745, 768]),\n",
       " 5194: array([  1, 378, 609, 769]),\n",
       " 360: array([236, 346, 548, 768]),\n",
       " 9534: array([  2, 425, 539, 768]),\n",
       " 3803: array([121, 296, 560, 768]),\n",
       " 6311: array([239, 378, 657, 768]),\n",
       " 6334: array([158, 387, 726, 768]),\n",
       " 2245: array([ 75, 400, 715, 768]),\n",
       " 8243: array([186, 366, 678, 768]),\n",
       " 10778: array([254, 414, 539, 768]),\n",
       " 10239: array([ 37, 476, 718, 768]),\n",
       " 6678: array([  0, 402, 609, 768]),\n",
       " 7248: array([121, 408, 512, 768]),\n",
       " 3143: array([224, 459, 724, 768]),\n",
       " 407: array([236, 280, 548, 768]),\n",
       " 9401: array([121, 272, 628, 768]),\n",
       " 2309: array([120, 500, 567, 768]),\n",
       " 642: array([224, 470, 621, 768]),\n",
       " 4065: array([211, 290, 539, 769]),\n",
       " 6801: array([  2, 378, 745, 768]),\n",
       " 5306: array([117, 332, 521, 768]),\n",
       " 24: array([117, 482, 528, 768]),\n",
       " 926: array([222, 286, 648, 768]),\n",
       " 10454: array([154, 313, 580, 768]),\n",
       " 467: array([  6, 377, 571, 786]),\n",
       " 1990: array([173, 387, 564, 768]),\n",
       " 4653: array([  6, 377, 571, 787]),\n",
       " 3878: array([114, 430, 521, 768]),\n",
       " 8485: array([114, 372, 567, 768]),\n",
       " 8218: array([140, 466, 686, 768]),\n",
       " 4310: array([140, 400, 579, 768]),\n",
       " 491: array([ 22, 306, 521, 768]),\n",
       " 5511: array([211, 290, 571, 768]),\n",
       " 6116: array([177, 503, 745, 768]),\n",
       " 5967: array([177, 305, 548, 768]),\n",
       " 2270: array([ 75, 290, 521, 768]),\n",
       " 6434: array([211, 290, 539, 770]),\n",
       " 6003: array([ 75, 290, 745, 768]),\n",
       " 4483: array([  2, 425, 559, 768]),\n",
       " 8559: array([211, 290, 539, 771]),\n",
       " 8730: array([254, 324, 715, 768]),\n",
       " 9460: array([126, 299, 547, 768]),\n",
       " 9456: array([ 75, 290, 685, 768]),\n",
       " 9592: array([211, 290, 539, 772]),\n",
       " 8644: array([173, 500, 688, 768]),\n",
       " 9884: array([ 32, 312, 629, 768]),\n",
       " 9253: array([245, 323, 556, 768]),\n",
       " 2617: array([ 70, 346, 628, 768]),\n",
       " 2618: array([224, 307, 721, 768]),\n",
       " 9250: array([ 75, 500, 521, 768]),\n",
       " 2619: array([253, 272, 688, 768]),\n",
       " 9251: array([ 14, 478, 645, 768]),\n",
       " 10698: array([  1, 274, 721, 768]),\n",
       " 10702: array([121, 495, 561, 768]),\n",
       " 10571: array([ 18, 265, 746, 768]),\n",
       " 4439: array([173, 503, 715, 768]),\n",
       " 3714: array([173, 434, 571, 768]),\n",
       " 10374: array([217, 466, 658, 768]),\n",
       " 10670: array([236, 289, 621, 768]),\n",
       " 8300: array([121, 390, 561, 768]),\n",
       " 2620: array([236, 260, 753, 768]),\n",
       " 5091: array([ 62, 378, 716, 768]),\n",
       " 10373: array([253, 320, 738, 768]),\n",
       " 6389: array([ 75, 371, 534, 768]),\n",
       " 10808: array([220, 264, 766, 768]),\n",
       " 5092: array([195, 286, 721, 768]),\n",
       " 5090: array([120, 341, 521, 768]),\n",
       " 5093: array([126, 396, 721, 768]),\n",
       " 10757: array([126, 396, 578, 768]),\n",
       " 11811: array([  1, 407, 548, 768]),\n",
       " 2054: array([211, 470, 745, 769]),\n",
       " 2311: array([  1, 290, 521, 768]),\n",
       " 11766: array([211, 290, 539, 773]),\n",
       " 11778: array([ 95, 470, 560, 768]),\n",
       " 11735: array([211, 470, 559, 768]),\n",
       " 6435: array([211, 470, 559, 769]),\n",
       " 11764: array([ 37, 348, 539, 768]),\n",
       " 12026: array([211, 470, 560, 769]),\n",
       " 11903: array([  2, 503, 539, 768]),\n",
       " 2733: array([  1, 290, 621, 768]),\n",
       " 11820: array([211, 500, 628, 768]),\n",
       " 2925: array([  2, 313, 571, 768]),\n",
       " 7818: array([  6, 377, 571, 788]),\n",
       " 3114: array([211, 470, 559, 770]),\n",
       " 3704: array([126, 431, 721, 768]),\n",
       " 4697: array([211, 460, 751, 768]),\n",
       " 9585: array([ 53, 296, 718, 768]),\n",
       " 6849: array([  6, 377, 571, 789]),\n",
       " 9590: array([ 74, 445, 716, 768]),\n",
       " 10561: array([  1, 346, 751, 768]),\n",
       " 10528: array([211, 470, 559, 771]),\n",
       " 3715: array([173, 314, 677, 768]),\n",
       " 10558: array([114, 393, 766, 768]),\n",
       " 10690: array([240, 278, 719, 768]),\n",
       " 11904: array([114, 302, 688, 768]),\n",
       " 10917: array([ 22, 394, 766, 768]),\n",
       " 10727: array([ 28, 431, 522, 768]),\n",
       " 11615: array([  2, 470, 521, 768]),\n",
       " 3604: array([120, 290, 628, 768]),\n",
       " 7136: array([200, 314, 724, 768]),\n",
       " 5325: array([224, 388, 677, 768]),\n",
       " 9784: array([211, 470, 745, 770]),\n",
       " 5394: array([211, 470, 534, 768]),\n",
       " 5186: array([ 28, 431, 522, 769]),\n",
       " 6495: array([173, 314, 726, 768]),\n",
       " 7724: array([ 28, 316, 609, 768]),\n",
       " 7652: array([126, 394, 646, 768]),\n",
       " 5802: array([ 75, 378, 751, 769]),\n",
       " 5326: array([  2, 470, 522, 768]),\n",
       " 6032: array([  2, 425, 623, 768]),\n",
       " 7532: array([ 95, 302, 715, 768]),\n",
       " 10004: array([211, 470, 560, 770]),\n",
       " 9735: array([ 95, 302, 720, 768]),\n",
       " 5519: array([  0, 277, 589, 768]),\n",
       " 8967: array([126, 470, 715, 768]),\n",
       " 2464: array([219, 440, 694, 768]),\n",
       " 6858: array([119, 419, 720, 768]),\n",
       " 6862: array([ 37, 302, 715, 768]),\n",
       " 6865: array([  9, 264, 645, 768]),\n",
       " 9387: array([ 75, 459, 662, 768]),\n",
       " 5522: array([229, 411, 715, 768]),\n",
       " 2513: array([138, 277, 631, 768]),\n",
       " 9688: array([126, 386, 734, 768]),\n",
       " 6402: array([211, 503, 751, 768]),\n",
       " 5023: array([211, 470, 609, 768]),\n",
       " 5977: array([  2, 377, 685, 768]),\n",
       " 6397: array([126, 378, 609, 768]),\n",
       " 5987: array([127, 261, 548, 768]),\n",
       " 9034: array([  6, 377, 571, 790]),\n",
       " 10632: array([ 37, 374, 641, 768]),\n",
       " 11186: array([ 53, 386, 641, 768]),\n",
       " 4549: array([ 70, 311, 703, 768]),\n",
       " 9077: array([205, 365, 615, 768]),\n",
       " 893: array([  2, 378, 751, 768]),\n",
       " 11411: array([140, 365, 763, 768]),\n",
       " 8635: array([ 95, 378, 567, 768]),\n",
       " 3563: array([ 28, 302, 628, 768]),\n",
       " 10106: array([177, 272, 560, 768]),\n",
       " 8150: array([200, 449, 716, 768]),\n",
       " 5132: array([126, 354, 560, 768]),\n",
       " 10435: array([211, 470, 720, 768]),\n",
       " 11249: array([211, 290, 685, 768]),\n",
       " 2038: array([126, 290, 652, 768]),\n",
       " 7130: array([ 28, 491, 609, 768]),\n",
       " 8227: array([211, 290, 539, 774]),\n",
       " 6461: array([ 75, 346, 745, 768]),\n",
       " 2308: array([193, 500, 644, 768]),\n",
       " 327: array([144, 388, 571, 768]),\n",
       " 11082: array([  6, 377, 571, 791]),\n",
       " 12001: array([236, 286, 641, 768]),\n",
       " 1904: array([236, 306, 719, 768]),\n",
       " 6360: array([  1, 377, 695, 768]),\n",
       " 5170: array([173, 377, 573, 768]),\n",
       " 10464: array([168, 434, 724, 768]),\n",
       " 227: array([156, 463, 512, 768]),\n",
       " 6804: array([ 32, 278, 726, 768]),\n",
       " 3897: array([177, 378, 675, 768]),\n",
       " 537: array([211, 377, 539, 768]),\n",
       " 6807: array([211, 470, 721, 768]),\n",
       " 11651: array([ 28, 423, 697, 768]),\n",
       " 11939: array([  1, 459, 720, 768]),\n",
       " 11874: array([  2, 459, 566, 768]),\n",
       " 4880: array([  6, 377, 571, 792]),\n",
       " 11791: array([219, 316, 761, 768]),\n",
       " 8124: array([ 28, 263, 539, 768]),\n",
       " 727: array([  1, 290, 751, 768]),\n",
       " 9763: array([211, 470, 559, 772]),\n",
       " 9620: array([220, 366, 579, 768]),\n",
       " 12066: array([168, 332, 720, 768]),\n",
       " 3426: array([ 28, 394, 548, 768]),\n",
       " 8851: array([  2, 503, 573, 768]),\n",
       " 9110: array([135, 460, 753, 768]),\n",
       " 9853: array([135, 346, 721, 768]),\n",
       " 9836: array([219, 346, 567, 768]),\n",
       " 10000: array([135, 346, 674, 768]),\n",
       " 10134: array([135, 346, 674, 769]),\n",
       " 8099: array([193, 377, 621, 768]),\n",
       " 11447: array([ 62, 445, 765, 768]),\n",
       " 11141: array([135, 413, 534, 768]),\n",
       " 10178: array([219, 316, 677, 768]),\n",
       " 11005: array([ 62, 445, 512, 768]),\n",
       " 2045: array([193, 272, 720, 768]),\n",
       " 5578: array([ 62, 295, 631, 768]),\n",
       " 10223: array([219, 377, 606, 768]),\n",
       " 11600: array([127, 450, 676, 768]),\n",
       " 9279: array([135, 367, 640, 768]),\n",
       " 917: array([135, 313, 559, 768]),\n",
       " 1492: array([127, 289, 512, 768]),\n",
       " 3081: array([177, 470, 641, 768]),\n",
       " 10746: array([135, 328, 648, 768]),\n",
       " 4263: array([239, 318, 745, 768]),\n",
       " 6388: array([ 84, 318, 541, 768]),\n",
       " 9083: array([193, 348, 720, 768]),\n",
       " 9088: array([178, 318, 580, 768]),\n",
       " 10335: array([220, 348, 644, 768]),\n",
       " 10356: array([  2, 470, 677, 768]),\n",
       " 55: array([ 28, 459, 631, 768]),\n",
       " 3163: array([  1, 434, 751, 768]),\n",
       " 10865: array([126, 459, 644, 768]),\n",
       " 8562: array([ 28, 272, 615, 768]),\n",
       " 3433: array([  1, 478, 521, 768]),\n",
       " 3928: array([150, 371, 696, 768]),\n",
       " 1254: array([150, 388, 761, 768]),\n",
       " 3071: array([126, 378, 745, 769]),\n",
       " 10667: array([  6, 377, 571, 793]),\n",
       " 8807: array([239, 316, 718, 768]),\n",
       " 4661: array([ 75, 470, 751, 768]),\n",
       " 6946: array([154, 295, 750, 768]),\n",
       " 1854: array([  6, 377, 571, 794]),\n",
       " 9244: array([211, 503, 564, 768]),\n",
       " 3013: array([168, 290, 715, 768]),\n",
       " 1690: array([219, 348, 621, 768]),\n",
       " 2577: array([ 75, 371, 751, 768]),\n",
       " 2423: array([  1, 400, 751, 768]),\n",
       " 53: array([211, 290, 720, 768]),\n",
       " 10428: array([173, 296, 721, 768]),\n",
       " 4712: array([  2, 408, 675, 768]),\n",
       " 2933: array([211, 470, 548, 768]),\n",
       " 11554: array([128, 360, 763, 768]),\n",
       " 1734: array([128, 394, 688, 768]),\n",
       " 8035: array([220, 491, 724, 768]),\n",
       " 10436: array([113, 312, 701, 768]),\n",
       " 595: array([128, 360, 560, 768]),\n",
       " 6142: array([  2, 408, 677, 768]),\n",
       " 4180: array([  2, 408, 745, 768]),\n",
       " 485: array([ 44, 398, 721, 768]),\n",
       " 3663: array([ 44, 478, 645, 768]),\n",
       " 10995: array([113, 365, 701, 768]),\n",
       " 11060: array([221, 436, 685, 768]),\n",
       " 772: array([ 26, 312, 646, 768]),\n",
       " 1930: array([126, 377, 564, 768]),\n",
       " 2639: array([243, 293, 564, 768]),\n",
       " 2501: array([117, 378, 521, 768]),\n",
       " 3927: array([ 75, 478, 539, 768]),\n",
       " 7218: array([177, 348, 670, 768]),\n",
       " 9568: array([211, 290, 539, 775]),\n",
       " 341: array([123, 390, 721, 768]),\n",
       " 8647: array([  1, 503, 718, 768]),\n",
       " 5257: array([211, 302, 644, 768]),\n",
       " 8279: array([  2, 470, 521, 769]),\n",
       " 2440: array([ 75, 313, 685, 768]),\n",
       " 6086: array([211, 470, 720, 769]),\n",
       " 4169: array([211, 470, 609, 769]),\n",
       " 8234: array([ 53, 289, 701, 768]),\n",
       " 6328: array([ 95, 476, 701, 769]),\n",
       " 3358: array([  2, 503, 559, 768]),\n",
       " 7419: array([ 95, 274, 560, 768]),\n",
       " 3421: array([128, 263, 560, 768]),\n",
       " 6953: array([120, 470, 716, 768]),\n",
       " 8575: array([211, 425, 718, 768]),\n",
       " 9600: array([ 28, 323, 720, 768]),\n",
       " 5106: array([ 28, 475, 623, 768]),\n",
       " 5107: array([121, 332, 606, 768]),\n",
       " 5188: array([144, 364, 522, 768]),\n",
       " 6484: array([ 53, 372, 559, 768]),\n",
       " 7317: array([ 53, 372, 609, 768]),\n",
       " 7614: array([160, 460, 694, 768]),\n",
       " 5533: array([126, 460, 661, 768]),\n",
       " 6458: array([186, 495, 747, 768]),\n",
       " 2175: array([186, 387, 522, 768]),\n",
       " 7479: array([211, 290, 539, 776]),\n",
       " 3113: array([173, 371, 564, 768]),\n",
       " 8416: array([177, 302, 720, 768]),\n",
       " 7972: array([  2, 503, 559, 769]),\n",
       " 10055: array([  6, 377, 571, 795]),\n",
       " 9292: array([211, 378, 720, 769]),\n",
       " 5858: array([  6, 377, 571, 796]),\n",
       " 4993: array([  6, 377, 571, 797]),\n",
       " 4665: array([156, 405, 652, 768]),\n",
       " 3463: array([ 80, 302, 664, 768]),\n",
       " 6523: array([ 95, 377, 623, 768]),\n",
       " 3018: array([211, 313, 559, 768]),\n",
       " 253: array([211, 503, 751, 769]),\n",
       " 8737: array([  1, 400, 719, 768]),\n",
       " 1544: array([  2, 503, 559, 770]),\n",
       " 555: array([ 28, 409, 560, 768]),\n",
       " 2549: array([ 74, 445, 644, 768]),\n",
       " 10416: array([156, 461, 697, 768]),\n",
       " 8346: array([173, 476, 567, 768]),\n",
       " 4672: array([ 27, 425, 560, 768]),\n",
       " 8738: array([224, 318, 718, 768]),\n",
       " 10731: array([193, 296, 623, 768]),\n",
       " 10827: array([ 32, 500, 715, 768]),\n",
       " 11248: array([ 37, 290, 580, 768]),\n",
       " 1468: array([211, 296, 751, 768]),\n",
       " 3602: array([126, 348, 560, 768]),\n",
       " 4955: array([ 95, 408, 753, 768]),\n",
       " 5843: array([ 75, 302, 623, 768]),\n",
       " 6486: array([ 72, 460, 583, 768]),\n",
       " 3594: array([121, 503, 677, 768]),\n",
       " 6462: array([ 53, 278, 745, 768]),\n",
       " 6465: array([ 22, 449, 539, 768]),\n",
       " 6487: array([  6, 377, 571, 798]),\n",
       " 6973: array([254, 419, 716, 768]),\n",
       " 6616: array([229, 345, 765, 768]),\n",
       " 7459: array([  9, 436, 579, 768]),\n",
       " 6403: array([135, 386, 571, 768]),\n",
       " 11923: array([114, 434, 662, 768]),\n",
       " 11894: array([200, 260, 677, 768]),\n",
       " 10839: array([185, 436, 661, 768]),\n",
       " 868: array([185, 409, 747, 768]),\n",
       " 3208: array([211, 302, 564, 768]),\n",
       " 739: array([222, 312, 718, 768]),\n",
       " 3848: array([254, 424, 521, 768]),\n",
       " 5293: array([156, 371, 644, 768]),\n",
       " 7512: array([121, 296, 685, 768]),\n",
       " 4023: array([  2, 408, 522, 768]),\n",
       " 2898: array([ 53, 500, 547, 768]),\n",
       " 6149: array([  2, 408, 522, 769]),\n",
       " 5197: array([120, 302, 720, 768]),\n",
       " 1915: array([ 18, 423, 716, 768]),\n",
       " 6700: array([150, 265, 541, 768]),\n",
       " 2722: array([ 13, 374, 629, 768]),\n",
       " 7003: array([ 35, 442, 716, 768]),\n",
       " 6208: array([114, 495, 695, 768]),\n",
       " 7937: array([ 28, 261, 522, 768]),\n",
       " 199: array([  2, 425, 559, 769]),\n",
       " 1464: array([ 22, 306, 571, 768]),\n",
       " 5384: array([114, 394, 715, 768]),\n",
       " 3209: array([121, 431, 766, 768]),\n",
       " 330: array([ 62, 388, 623, 768]),\n",
       " 5535: array([245, 346, 753, 768]),\n",
       " 5954: array([  2, 371, 750, 768]),\n",
       " 2022: array([121, 290, 653, 768]),\n",
       " 4056: array([166, 269, 594, 768]),\n",
       " 4187: array([  6, 377, 571, 799]),\n",
       " 6869: array([ 75, 290, 745, 769]),\n",
       " 1768: array([166, 269, 738, 768]),\n",
       " 7768: array([121, 346, 662, 768]),\n",
       " 2708: array([121, 470, 560, 768]),\n",
       " 7001: array([ 75, 470, 574, 768]),\n",
       " 770: array([236, 312, 724, 768]),\n",
       " 3036: array([205, 274, 539, 768]),\n",
       " 1802: array([ 22, 400, 567, 768]),\n",
       " 720: array([ 75, 260, 522, 768]),\n",
       " 6359: array([173, 296, 573, 768]),\n",
       " 9633: array([ 75, 503, 745, 768]),\n",
       " 7565: array([224, 341, 522, 768]),\n",
       " 6673: array([ 73, 510, 726, 768]),\n",
       " 1601: array([128, 302, 628, 768]),\n",
       " 680: array([ 53, 277, 719, 768]),\n",
       " 4475: array([ 53, 511, 701, 768]),\n",
       " 6965: array([114, 386, 697, 768]),\n",
       " 8784: array([193, 332, 701, 768]),\n",
       " 8785: array([168, 290, 753, 768]),\n",
       " 844: array([138, 277, 629, 768]),\n",
       " 1507: array([ 75, 431, 559, 768]),\n",
       " 1508: array([173, 431, 720, 768]),\n",
       " 3066: array([ 42, 348, 573, 768]),\n",
       " 9035: array([ 80, 306, 567, 768]),\n",
       " 9584: array([200, 460, 621, 768]),\n",
       " 3182: array([ 53, 274, 716, 768]),\n",
       " 3348: array([ 53, 274, 559, 768]),\n",
       " 9587: array([ 53, 323, 761, 768]),\n",
       " 9595: array([  6, 377, 571, 800]),\n",
       " 10699: array([236, 311, 746, 768]),\n",
       " 10812: array([120, 503, 522, 768]),\n",
       " 9086: array([ 37, 261, 559, 768]),\n",
       " 10896: array([ 53, 393, 701, 768]),\n",
       " 10669: array([150, 407, 580, 768]),\n",
       " 10164: array([ 62, 489, 765, 768]),\n",
       " 8977: array([  2, 388, 559, 768]),\n",
       " 10752: array([211, 470, 609, 770]),\n",
       " 11054: array([211, 296, 522, 769]),\n",
       " 10753: array([  2, 425, 674, 768]),\n",
       " 8976: array([211, 290, 539, 777]),\n",
       " 1291: array([ 95, 503, 571, 768]),\n",
       " 11771: array([  2, 470, 560, 768]),\n",
       " 11765: array([  1, 400, 751, 769]),\n",
       " 11802: array([  2, 503, 720, 768]),\n",
       " 11803: array([ 32, 494, 641, 768]),\n",
       " 3549: array([211, 313, 559, 769]),\n",
       " 9561: array([ 75, 503, 745, 769]),\n",
       " 6433: array([  1, 503, 677, 768]),\n",
       " 11780: array([ 95, 302, 715, 769]),\n",
       " 323: array([  1, 290, 697, 768]),\n",
       " 10950: array([  1, 407, 573, 768]),\n",
       " 2472: array([ 75, 470, 628, 768]),\n",
       " 1927: array([211, 378, 522, 768]),\n",
       " 103: array([205, 328, 512, 768]),\n",
       " 263: array([  6, 377, 571, 801]),\n",
       " 553: array([171, 290, 534, 768]),\n",
       " 3773: array([211, 470, 539, 768]),\n",
       " 7847: array([224, 503, 745, 768]),\n",
       " 8538: array([ 37, 482, 716, 768]),\n",
       " 8954: array([220, 445, 573, 768]),\n",
       " 7132: array([135, 390, 548, 768]),\n",
       " 4996: array([ 62, 291, 560, 768]),\n",
       " 10565: array([135, 360, 695, 768]),\n",
       " 6210: array([ 70, 305, 574, 768]),\n",
       " 3279: array([211, 460, 745, 768]),\n",
       " 909: array([169, 499, 751, 768]),\n",
       " 1121: array([193, 348, 750, 768]),\n",
       " 5243: array([177, 491, 564, 768]),\n",
       " 9084: array([  0, 303, 657, 768]),\n",
       " 8026: array([211, 290, 720, 769]),\n",
       " 8153: array([126, 478, 560, 768]),\n",
       " 2972: array([ 70, 309, 646, 768]),\n",
       " 9380: array([219, 424, 589, 768]),\n",
       " 1522: array([211, 470, 609, 771]),\n",
       " 1827: array([ 53, 470, 761, 768]),\n",
       " 717: array([114, 409, 720, 768]),\n",
       " 11452: array([119, 449, 696, 768]),\n",
       " 4898: array([227, 341, 761, 768]),\n",
       " 6735: array([121, 302, 761, 768]),\n",
       " 1875: array([ 72, 460, 645, 768]),\n",
       " 7210: array([166, 263, 573, 768]),\n",
       " 8889: array([227, 414, 655, 768]),\n",
       " 8044: array([ 26, 510, 559, 768]),\n",
       " 9196: array([ 18, 414, 750, 768]),\n",
       " 9952: array([205, 346, 655, 768]),\n",
       " 9189: array([205, 346, 696, 768]),\n",
       " 4367: array([ 62, 299, 526, 768]),\n",
       " 10016: array([121, 299, 721, 768]),\n",
       " 3541: array([156, 286, 685, 768]),\n",
       " 3654: array([121, 290, 653, 769]),\n",
       " 9266: array([120, 387, 522, 768]),\n",
       " 4868: array([177, 311, 750, 768]),\n",
       " 11403: array([ 75, 290, 621, 768]),\n",
       " 10385: array([ 37, 489, 669, 768]),\n",
       " 8808: array([ 75, 265, 685, 768]),\n",
       " 3050: array([  1, 302, 715, 768]),\n",
       " 9883: array([ 37, 313, 609, 768]),\n",
       " 11664: array([  1, 459, 716, 768]),\n",
       " 12033: array([  1, 388, 534, 768]),\n",
       " 1577: array([ 18, 494, 641, 768]),\n",
       " 3780: array([177, 470, 560, 768]),\n",
       " 3167: array([  1, 470, 560, 768]),\n",
       " 4441: array([126, 274, 559, 768]),\n",
       " 6770: array([ 37, 277, 609, 768]),\n",
       " 4978: array([156, 277, 589, 768]),\n",
       " 6743: array([  2, 459, 648, 768]),\n",
       " 3362: array([140, 323, 560, 768]),\n",
       " 8380: array([ 75, 511, 560, 768]),\n",
       " 3763: array([ 26, 459, 670, 768]),\n",
       " 4024: array([ 32, 394, 751, 768]),\n",
       " 9309: array([  0, 499, 688, 768]),\n",
       " 10282: array([ 37, 499, 644, 768]),\n",
       " 10560: array([ 37, 394, 522, 768]),\n",
       " 11767: array([ 37, 274, 688, 768]),\n",
       " 10506: array([ 37, 500, 628, 768]),\n",
       " 2662: array([ 37, 408, 609, 768]),\n",
       " 2689: array([224, 313, 522, 768]),\n",
       " 9728: array([224, 377, 661, 768]),\n",
       " 5007: array([ 75, 470, 751, 769]),\n",
       " 9477: array([ 37, 263, 605, 768]),\n",
       " 9468: array([ 75, 302, 628, 768]),\n",
       " 248: array([ 37, 314, 628, 768]),\n",
       " 2884: array([224, 377, 661, 769]),\n",
       " 5677: array([224, 377, 661, 770]),\n",
       " 163: array([ 37, 314, 548, 768]),\n",
       " 10689: array([224, 503, 761, 768]),\n",
       " 11710: array([ 37, 503, 745, 768]),\n",
       " 5245: array([ 37, 371, 559, 768]),\n",
       " 10693: array([ 37, 274, 560, 768]),\n",
       " 8820: array([ 75, 408, 628, 768]),\n",
       " 1593: array([186, 394, 641, 768]),\n",
       " 11370: array([211, 470, 560, 771]),\n",
       " 8092: array([211, 316, 745, 768]),\n",
       " 11375: array([114, 436, 583, 768]),\n",
       " 10512: array([  6, 377, 571, 802]),\n",
       " 2468: array([222, 333, 641, 768]),\n",
       " 2987: array([  6, 377, 571, 803]),\n",
       " 4911: array([173, 302, 662, 768]),\n",
       " 10291: array([120, 427, 539, 768]),\n",
       " 6981: array([114, 333, 750, 768]),\n",
       " 4910: array([211, 470, 559, 773]),\n",
       " 3691: array([  6, 377, 571, 804]),\n",
       " 3989: array([211, 470, 609, 772]),\n",
       " 2032: array([156, 449, 580, 768]),\n",
       " 47: array([224, 460, 720, 768]),\n",
       " 8313: array([173, 500, 521, 768]),\n",
       " 3252: array([173, 431, 761, 768]),\n",
       " 7215: array([  1, 286, 701, 768]),\n",
       " 7523: array([  1, 302, 548, 768]),\n",
       " 2037: array([201, 460, 559, 768]),\n",
       " 4914: array([ 32, 289, 631, 768]),\n",
       " 5352: array([ 18, 409, 621, 768]),\n",
       " 5030: array([ 62, 346, 658, 768]),\n",
       " 5564: array([ 32, 348, 697, 768]),\n",
       " 10740: array([126, 408, 715, 768]),\n",
       " 6302: array([156, 479, 765, 768]),\n",
       " 1118: array([ 53, 400, 701, 768]),\n",
       " 3990: array([186, 367, 525, 768]),\n",
       " 546: array([  1, 302, 715, 769]),\n",
       " 1085: array([173, 372, 573, 768]),\n",
       " 5096: array([  6, 377, 571, 805]),\n",
       " 837: array([ 18, 402, 560, 768]),\n",
       " 7660: array([ 73, 510, 649, 768]),\n",
       " 5168: array([156, 408, 765, 768]),\n",
       " 10442: array([140, 460, 548, 768]),\n",
       " 2573: array([177, 313, 512, 768]),\n",
       " 6765: array([177, 302, 715, 769]),\n",
       " 818: array([135, 313, 621, 768]),\n",
       " 6132: array([211, 470, 560, 772]),\n",
       " 1057: array([211, 313, 560, 768]),\n",
       " 281: array([183, 408, 571, 768]),\n",
       " 4916: array([  6, 377, 571, 806]),\n",
       " 3769: array([ 37, 305, 685, 768]),\n",
       " 11928: array([173, 431, 564, 768]),\n",
       " 6733: array([126, 407, 716, 768]),\n",
       " 1354: array([228, 369, 548, 768]),\n",
       " 2811: array([128, 388, 751, 768]),\n",
       " 71: array([ 28, 408, 675, 768]),\n",
       " 1352: array([139, 295, 751, 768]),\n",
       " 10382: array([211, 470, 761, 768]),\n",
       " 1262: array([  2, 332, 685, 768]),\n",
       " 2558: array([139, 295, 751, 769]),\n",
       " 11216: array([233, 413, 560, 768]),\n",
       " 11711: array([211, 470, 559, 774]),\n",
       " 11220: array([ 74, 409, 695, 768]),\n",
       " 11770: array([ 95, 269, 525, 768]),\n",
       " 11712: array([211, 296, 522, 770]),\n",
       " 11708: array([  2, 425, 677, 768]),\n",
       " 4418: array([ 32, 299, 753, 768]),\n",
       " 1697: array([  6, 377, 571, 807]),\n",
       " 3945: array([ 37, 261, 685, 768]),\n",
       " 7007: array([211, 470, 655, 768]),\n",
       " 4618: array([ 75, 261, 595, 768]),\n",
       " 849: array([211, 286, 685, 768]),\n",
       " 899: array([157, 431, 606, 768]),\n",
       " 2009: array([158, 369, 644, 768]),\n",
       " 6452: array([ 75, 378, 522, 768]),\n",
       " 3200: array([177, 318, 522, 768]),\n",
       " 2374: array([ 44, 272, 652, 768]),\n",
       " 6768: array([169, 390, 623, 768]),\n",
       " 11385: array([ 98, 489, 539, 768]),\n",
       " 8786: array([ 75, 500, 648, 768]),\n",
       " 268: array([  2, 503, 719, 768]),\n",
       " 3743: array([211, 290, 539, 778]),\n",
       " 9847: array([  2, 346, 716, 768]),\n",
       " 10797: array([224, 274, 747, 768]),\n",
       " 7477: array([186, 300, 571, 768]),\n",
       " 9404: array([  2, 408, 745, 769]),\n",
       " 721: array([135, 295, 645, 768]),\n",
       " 4032: array([ 75, 296, 745, 768]),\n",
       " 2377: array([ 53, 261, 574, 768]),\n",
       " 7366: array([  6, 377, 571, 808]),\n",
       " 2608: array([177, 290, 641, 768]),\n",
       " 2764: array([ 37, 425, 644, 768]),\n",
       " 3758: array([224, 503, 548, 768]),\n",
       " 7181: array([224, 377, 522, 768]),\n",
       " 8472: array([224, 378, 522, 768]),\n",
       " 4654: array([224, 377, 685, 768]),\n",
       " 8486: array([220, 463, 655, 768]),\n",
       " 4985: array([ 75, 302, 564, 768]),\n",
       " 3504: array([254, 450, 751, 768]),\n",
       " 8129: array([ 97, 274, 621, 768]),\n",
       " 7535: array([239, 358, 534, 768]),\n",
       " 10245: array([ 97, 510, 615, 768]),\n",
       " 1001: array([ 97, 323, 697, 768]),\n",
       " 9921: array([214, 502, 761, 768]),\n",
       " 5362: array([211, 470, 560, 773]),\n",
       " 5635: array([126, 316, 753, 768]),\n",
       " 2983: array([167, 346, 571, 768]),\n",
       " 5697: array([168, 290, 761, 768]),\n",
       " 11784: array([219, 372, 609, 768]),\n",
       " 7172: array([ 84, 503, 560, 768]),\n",
       " 10481: array([169, 369, 751, 768]),\n",
       " 429: array([168, 263, 677, 768]),\n",
       " 6138: array([187, 265, 548, 768]),\n",
       " 5413: array([187, 482, 585, 768]),\n",
       " 7096: array([168, 409, 580, 768]),\n",
       " 159: array([187, 289, 688, 768]),\n",
       " 4336: array([168, 265, 585, 768]),\n",
       " 1585: array([187, 434, 715, 768]),\n",
       " 4189: array([187, 387, 716, 768]),\n",
       " 5501: array([219, 407, 580, 768]),\n",
       " 1912: array([171, 354, 745, 768]),\n",
       " 2076: array([ 84, 323, 522, 768]),\n",
       " 3699: array([168, 393, 745, 768]),\n",
       " 10570: array([168, 312, 609, 768]),\n",
       " 3195: array([168, 434, 745, 768]),\n",
       " 5783: array([168, 290, 715, 769]),\n",
       " 4271: array([ 84, 434, 621, 768]),\n",
       " 5831: array([171, 409, 746, 768]),\n",
       " 483: array([171, 332, 609, 768]),\n",
       " 7095: array([171, 495, 579, 768]),\n",
       " 5878: array([  6, 377, 571, 809]),\n",
       " 3832: array([219, 316, 560, 769]),\n",
       " 2200: array([187, 434, 522, 768]),\n",
       " 6613: array([168, 407, 719, 768]),\n",
       " 9392: array([187, 434, 595, 768]),\n",
       " 2028: array([ 84, 312, 609, 768]),\n",
       " 1274: array([193, 503, 762, 768]),\n",
       " 9966: array([219, 500, 641, 768]),\n",
       " 5881: array([193, 309, 641, 768]),\n",
       " 10573: array([168, 290, 753, 769]),\n",
       " 1213: array([168, 290, 753, 770]),\n",
       " 3596: array([ 84, 408, 753, 768]),\n",
       " 7689: array([219, 510, 675, 768]),\n",
       " 6906: array([ 84, 312, 561, 768]),\n",
       " 10696: array([193, 396, 724, 768]),\n",
       " 4691: array([ 37, 377, 750, 768]),\n",
       " 1294: array([ 95, 394, 661, 768]),\n",
       " 6528: array([ 95, 377, 559, 768]),\n",
       " 10384: array([138, 299, 685, 768]),\n",
       " 9360: array([138, 416, 674, 768]),\n",
       " 11102: array([  6, 377, 571, 810]),\n",
       " 847: array([  2, 305, 631, 768]),\n",
       " 11312: array([ 75, 378, 719, 768]),\n",
       " 11311: array([ 95, 371, 560, 768]),\n",
       " 5200: array([  6, 377, 571, 811]),\n",
       " 4450: array([ 95, 371, 628, 768]),\n",
       " 2506: array([236, 430, 534, 768]),\n",
       " 4510: array([ 75, 503, 628, 768]),\n",
       " 11922: array([224, 290, 560, 768]),\n",
       " 487: array([ 95, 323, 688, 768]),\n",
       " 8250: array([ 75, 394, 609, 768]),\n",
       " 10133: array([123, 460, 641, 768]),\n",
       " 8958: array([ 98, 377, 628, 768]),\n",
       " 326: array([ 75, 296, 559, 768]),\n",
       " 10616: array([  6, 377, 571, 812]),\n",
       " 7930: array([ 37, 377, 677, 768]),\n",
       " 9999: array([211, 408, 753, 768]),\n",
       " 11271: array([254, 290, 719, 768]),\n",
       " 2547: array([254, 414, 646, 768]),\n",
       " 8074: array([  2, 470, 628, 768]),\n",
       " 10706: array([113, 365, 589, 768]),\n",
       " 7911: array([  2, 290, 715, 768]),\n",
       " 10566: array([138, 350, 761, 768]),\n",
       " 8556: array([168, 313, 655, 768]),\n",
       " 10538: array([ 53, 365, 520, 768]),\n",
       " 267: array([ 73, 449, 515, 768]),\n",
       " 7807: array([185, 328, 715, 768]),\n",
       " 7135: array([127, 274, 718, 768]),\n",
       " 4016: array([185, 386, 747, 768]),\n",
       " 2673: array([ 98, 378, 548, 768]),\n",
       " 7881: array([ 98, 400, 695, 768]),\n",
       " 7880: array([  6, 377, 571, 813]),\n",
       " 11512: array([211, 503, 720, 768]),\n",
       " 6237: array([221, 360, 583, 768]),\n",
       " 10108: array([211, 470, 522, 769]),\n",
       " 9990: array([211, 470, 534, 769]),\n",
       " 404: array([173, 408, 675, 768]),\n",
       " 2128: array([173, 503, 745, 768]),\n",
       " 6366: array([  2, 503, 512, 768]),\n",
       " 7991: array([ 42, 459, 662, 768]),\n",
       " 8649: array([  6, 377, 571, 814]),\n",
       " 11300: array([156, 376, 525, 768]),\n",
       " 11287: array([156, 277, 629, 768]),\n",
       " 8744: array([185, 419, 664, 768]),\n",
       " 7743: array([  6, 377, 571, 815]),\n",
       " 11345: array([ 98, 289, 718, 768]),\n",
       " 11292: array([ 28, 459, 560, 768]),\n",
       " 4268: array([  2, 425, 539, 769]),\n",
       " 11338: array([ 74, 460, 571, 768]),\n",
       " 5343: array([  1, 470, 560, 769]),\n",
       " 2720: array([150, 500, 629, 768]),\n",
       " 1240: array([ 75, 459, 688, 768]),\n",
       " 5528: array([ 54, 369, 750, 768]),\n",
       " 6048: array([ 95, 377, 715, 768]),\n",
       " 1027: array([ 37, 296, 621, 768]),\n",
       " 3708: array([ 32, 400, 534, 768]),\n",
       " 7700: array([ 76, 293, 657, 768]),\n",
       " 5143: array([ 75, 470, 522, 768]),\n",
       " 5973: array([224, 346, 573, 768]),\n",
       " 7226: array([ 54, 372, 669, 768]),\n",
       " 5115: array([224, 346, 573, 769]),\n",
       " 7698: array([  2, 377, 573, 768]),\n",
       " 7701: array([  6, 377, 571, 816]),\n",
       " 8814: array([121, 470, 559, 768]),\n",
       " 2007: array([245, 274, 560, 768]),\n",
       " 7717: array([ 68, 477, 520, 768]),\n",
       " 7392: array([121, 290, 628, 768]),\n",
       " 6868: array([  1, 316, 539, 768]),\n",
       " 880: array([ 75, 371, 560, 768]),\n",
       " 8711: array([ 68, 477, 513, 768]),\n",
       " 5788: array([ 68, 427, 548, 768]),\n",
       " 6071: array([121, 419, 628, 768]),\n",
       " 5790: array([ 98, 413, 579, 768]),\n",
       " 2215: array([121, 371, 574, 768]),\n",
       " 5791: array([121, 419, 721, 768]),\n",
       " 220: array([ 75, 503, 751, 768]),\n",
       " 7448: array([ 28, 460, 719, 768]),\n",
       " 4435: array([236, 394, 670, 768]),\n",
       " 5786: array([193, 332, 750, 768]),\n",
       " 5792: array([126, 407, 659, 768]),\n",
       " 3002: array([ 95, 431, 580, 768]),\n",
       " 5794: array([243, 293, 513, 768]),\n",
       " 2605: array([ 95, 312, 609, 768]),\n",
       " 3222: array([  6, 377, 571, 817]),\n",
       " 7795: array([144, 390, 719, 768]),\n",
       " 6916: array([  9, 264, 645, 769]),\n",
       " 2360: array([156, 348, 589, 768]),\n",
       " 7538: array([177, 274, 580, 768]),\n",
       " 2793: array([186, 302, 649, 768]),\n",
       " 6760: array([178, 410, 721, 768]),\n",
       " 8625: array([ 75, 503, 571, 768]),\n",
       " 8766: array([  2, 290, 539, 768]),\n",
       " 8139: array([  2, 378, 621, 768]),\n",
       " 9203: array([233, 387, 756, 768]),\n",
       " 2348: array([233, 413, 688, 768]),\n",
       " 3713: array([156, 358, 541, 768]),\n",
       " 8723: array([ 37, 503, 720, 768]),\n",
       " 8724: array([135, 440, 649, 768]),\n",
       " 9202: array([  6, 377, 571, 818]),\n",
       " 6782: array([ 84, 323, 522, 769]),\n",
       " 821: array([ 75, 470, 609, 768]),\n",
       " 9204: array([193, 407, 716, 768]),\n",
       " 10646: array([192, 272, 560, 768]),\n",
       " 828: array([185, 489, 657, 768]),\n",
       " 1093: array([ 32, 348, 745, 769]),\n",
       " 7860: array([ 75, 390, 521, 768]),\n",
       " 2249: array([173, 302, 548, 768]),\n",
       " 11563: array([  6, 377, 571, 819]),\n",
       " 771: array([235, 348, 678, 768]),\n",
       " 1886: array([211, 470, 688, 768]),\n",
       " 11244: array([ 53, 296, 644, 768]),\n",
       " 6960: array([  1, 377, 609, 768]),\n",
       " 9723: array([120, 371, 623, 768]),\n",
       " 8198: array([211, 372, 761, 768]),\n",
       " 2648: array([128, 274, 751, 768]),\n",
       " 10246: array([169, 427, 605, 768]),\n",
       " 2337: array([185, 476, 522, 768]),\n",
       " 3802: array([128, 470, 628, 768]),\n",
       " 4050: array([  2, 408, 721, 768]),\n",
       " 4052: array([ 70, 348, 716, 768]),\n",
       " 5348: array([ 70, 400, 573, 768]),\n",
       " 11052: array([121, 378, 719, 768]),\n",
       " 41: array([  0, 265, 606, 768]),\n",
       " 2149: array([  2, 503, 559, 771]),\n",
       " 3952: array([211, 377, 559, 768]),\n",
       " 3160: array([ 75, 470, 521, 768]),\n",
       " 10315: array([ 75, 296, 559, 769]),\n",
       " 9722: array([211, 332, 720, 768]),\n",
       " 5457: array([ 37, 470, 715, 768]),\n",
       " 1530: array([173, 470, 548, 768]),\n",
       " 9875: array([ 75, 470, 609, 769]),\n",
       " 10862: array([ 42, 470, 719, 768]),\n",
       " 639: array([ 37, 387, 512, 768]),\n",
       " 9998: array([ 42, 332, 762, 768]),\n",
       " 8076: array([ 75, 470, 548, 768]),\n",
       " 7757: array([ 37, 387, 609, 768]),\n",
       " 7424: array([  1, 460, 751, 768]),\n",
       " 5263: array([ 75, 277, 761, 768]),\n",
       " 5262: array([ 37, 346, 548, 768]),\n",
       " 7141: array([ 37, 503, 745, 769]),\n",
       " 6329: array([ 37, 387, 621, 768]),\n",
       " 8980: array([ 75, 378, 609, 768]),\n",
       " 9876: array([ 42, 290, 685, 768]),\n",
       " 3746: array([ 28, 377, 631, 768]),\n",
       " 1869: array([ 37, 459, 623, 768]),\n",
       " 1989: array([224, 378, 623, 768]),\n",
       " 9744: array([  1, 302, 685, 768]),\n",
       " 750: array([ 75, 290, 751, 768]),\n",
       " 5418: array([ 37, 305, 539, 768]),\n",
       " 3844: array([ 18, 394, 715, 768]),\n",
       " 2299: array([ 75, 378, 751, 770]),\n",
       " 3159: array([ 75, 431, 621, 768]),\n",
       " 4369: array([245, 394, 560, 768]),\n",
       " 9471: array([ 11, 394, 674, 768]),\n",
       " 9546: array([ 53, 511, 745, 768]),\n",
       " 114: array([135, 511, 761, 768]),\n",
       " 6192: array([173, 413, 746, 768]),\n",
       " 7330: array([144, 277, 685, 768]),\n",
       " 7244: array([236, 290, 715, 768]),\n",
       " 2339: array([224, 323, 675, 768]),\n",
       " 6451: array([  6, 377, 571, 820]),\n",
       " 2060: array([160, 400, 745, 768]),\n",
       " 6683: array([  1, 425, 512, 768]),\n",
       " 7475: array([173, 377, 521, 768]),\n",
       " 7883: array([ 53, 470, 559, 769]),\n",
       " 8108: array([177, 377, 745, 768]),\n",
       " 9172: array([  2, 378, 560, 768]),\n",
       " 10710: array([128, 459, 641, 768]),\n",
       " 9577: array([  2, 459, 657, 768]),\n",
       " 11322: array([ 28, 421, 747, 768]),\n",
       " 5809: array([168, 265, 560, 768]),\n",
       " 1586: array([168, 482, 621, 768]),\n",
       " 4018: array([219, 316, 560, 770]),\n",
       " 3576: array([  6, 377, 571, 821]),\n",
       " 10503: array([ 75, 388, 686, 768]),\n",
       " 8790: array([ 75, 470, 548, 769]),\n",
       " 10498: array([173, 377, 559, 768]),\n",
       " 2319: array([ 37, 503, 522, 768]),\n",
       " 254: array([173, 377, 721, 768]),\n",
       " 408: array([ 75, 460, 559, 768]),\n",
       " 7302: array([ 74, 332, 738, 768]),\n",
       " 8020: array([211, 296, 745, 768]),\n",
       " ...}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "with open('/home/jovyan/gusak/semantic_seqrec/data/item_sem_id_modified.pkl', 'rb') as f:\n",
    "    item_sem_id = pkl.load(f)\n",
    "\n",
    "item_sem_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(104, 431, 695, 768): 9450,\n",
       " (211, 332, 621, 768): 9840,\n",
       " (193, 307, 697, 768): 10077,\n",
       " (84, 430, 657, 768): 11156,\n",
       " (193, 436, 645, 768): 11753,\n",
       " (1, 295, 720, 768): 11864,\n",
       " (120, 291, 761, 768): 3310,\n",
       " (235, 506, 751, 768): 4573,\n",
       " (177, 323, 750, 768): 4137,\n",
       " (177, 323, 677, 768): 9080,\n",
       " (140, 396, 585, 768): 10956,\n",
       " (15, 386, 559, 768): 4387,\n",
       " (15, 269, 696, 768): 6363,\n",
       " (254, 289, 674, 768): 454,\n",
       " (92, 463, 696, 768): 4209,\n",
       " (107, 300, 715, 768): 59,\n",
       " (211, 470, 675, 768): 5666,\n",
       " (2, 434, 724, 768): 8650,\n",
       " (6, 377, 571, 768): 9405,\n",
       " (1, 378, 609, 768): 8877,\n",
       " (37, 314, 743, 768): 8824,\n",
       " (37, 374, 573, 768): 8651,\n",
       " (75, 378, 751, 768): 10131,\n",
       " (1, 508, 688, 768): 10254,\n",
       " (1, 378, 548, 768): 8969,\n",
       " (211, 378, 751, 768): 5098,\n",
       " (74, 348, 644, 768): 2714,\n",
       " (2, 408, 512, 768): 10321,\n",
       " (60, 290, 751, 768): 10163,\n",
       " (2, 500, 644, 768): 8072,\n",
       " (200, 427, 645, 768): 9767,\n",
       " (200, 365, 761, 768): 3496,\n",
       " (60, 277, 697, 768): 7450,\n",
       " (74, 296, 675, 768): 8030,\n",
       " (211, 378, 720, 768): 9622,\n",
       " (126, 388, 609, 768): 10228,\n",
       " (2, 388, 750, 768): 8936,\n",
       " (2, 378, 720, 768): 6005,\n",
       " (2, 489, 716, 768): 3615,\n",
       " (60, 424, 621, 768): 9212,\n",
       " (60, 408, 697, 768): 4644,\n",
       " (2, 431, 720, 768): 8662,\n",
       " (211, 296, 522, 768): 3906,\n",
       " (60, 378, 609, 768): 9581,\n",
       " (200, 424, 761, 768): 10148,\n",
       " (2, 313, 715, 768): 8347,\n",
       " (6, 377, 571, 769): 4277,\n",
       " (32, 470, 628, 768): 9857,\n",
       " (177, 378, 674, 768): 2939,\n",
       " (177, 378, 641, 768): 10926,\n",
       " (211, 470, 522, 768): 9885,\n",
       " (83, 289, 585, 768): 3768,\n",
       " (83, 289, 585, 769): 3082,\n",
       " (6, 377, 571, 770): 599,\n",
       " (6, 377, 571, 771): 4185,\n",
       " (6, 377, 571, 772): 2420,\n",
       " (60, 378, 720, 768): 9201,\n",
       " (53, 470, 559, 768): 1178,\n",
       " (53, 302, 573, 768): 948,\n",
       " (177, 423, 670, 768): 457,\n",
       " (44, 378, 670, 768): 2922,\n",
       " (6, 377, 571, 773): 5381,\n",
       " (2, 470, 574, 768): 3000,\n",
       " (2, 289, 559, 768): 3905,\n",
       " (128, 400, 721, 768): 7560,\n",
       " (187, 312, 745, 768): 3904,\n",
       " (44, 431, 720, 768): 7140,\n",
       " (158, 394, 605, 768): 7857,\n",
       " (208, 354, 571, 768): 6607,\n",
       " (6, 377, 571, 774): 9560,\n",
       " (6, 377, 571, 775): 8608,\n",
       " (6, 377, 571, 776): 10266,\n",
       " (6, 377, 571, 777): 1521,\n",
       " (220, 296, 720, 768): 3896,\n",
       " (1, 478, 719, 768): 6606,\n",
       " (128, 510, 655, 768): 7849,\n",
       " (15, 386, 670, 768): 8613,\n",
       " (6, 377, 571, 778): 7397,\n",
       " (92, 463, 696, 769): 10042,\n",
       " (2, 394, 697, 768): 7789,\n",
       " (236, 479, 762, 768): 7862,\n",
       " (13, 299, 677, 768): 5181,\n",
       " (228, 345, 718, 768): 8482,\n",
       " (211, 378, 695, 768): 5850,\n",
       " (54, 405, 560, 768): 10322,\n",
       " (92, 354, 746, 768): 8466,\n",
       " (15, 307, 548, 768): 10364,\n",
       " (1, 500, 695, 768): 7524,\n",
       " (54, 377, 561, 768): 7922,\n",
       " (211, 312, 559, 768): 3094,\n",
       " (126, 503, 560, 768): 9494,\n",
       " (208, 402, 521, 768): 10842,\n",
       " (44, 378, 751, 768): 7042,\n",
       " (211, 503, 573, 768): 1219,\n",
       " (28, 354, 621, 768): 455,\n",
       " (211, 470, 745, 768): 10113,\n",
       " (211, 459, 745, 768): 278,\n",
       " (1, 277, 721, 768): 10094,\n",
       " (239, 419, 746, 768): 3227,\n",
       " (95, 378, 719, 768): 8043,\n",
       " (211, 470, 560, 768): 10574,\n",
       " (2, 277, 721, 768): 3531,\n",
       " (2, 305, 539, 768): 5024,\n",
       " (219, 316, 560, 768): 4196,\n",
       " (177, 302, 715, 768): 6969,\n",
       " (44, 316, 629, 768): 4235,\n",
       " (83, 470, 715, 768): 5063,\n",
       " (74, 478, 766, 768): 4195,\n",
       " (2, 348, 719, 768): 10580,\n",
       " (158, 378, 573, 768): 7298,\n",
       " (15, 286, 685, 768): 11349,\n",
       " (15, 378, 720, 768): 11234,\n",
       " (1, 371, 567, 768): 1190,\n",
       " (126, 409, 751, 768): 8748,\n",
       " (28, 459, 628, 768): 3313,\n",
       " (2, 388, 662, 768): 6231,\n",
       " (83, 470, 559, 768): 11793,\n",
       " (219, 462, 766, 768): 5095,\n",
       " (60, 302, 628, 768): 5527,\n",
       " (177, 318, 674, 768): 187,\n",
       " (177, 318, 721, 768): 3514,\n",
       " (221, 345, 580, 768): 10287,\n",
       " (138, 476, 652, 768): 8199,\n",
       " (120, 410, 751, 768): 1675,\n",
       " (2, 272, 512, 768): 664,\n",
       " (32, 371, 628, 768): 2244,\n",
       " (140, 495, 751, 768): 2646,\n",
       " (6, 377, 571, 779): 7224,\n",
       " (28, 503, 655, 768): 8844,\n",
       " (60, 348, 559, 768): 11361,\n",
       " (28, 378, 697, 768): 13,\n",
       " (211, 503, 522, 768): 6281,\n",
       " (211, 272, 701, 768): 9432,\n",
       " (2, 459, 609, 768): 7122,\n",
       " (138, 277, 745, 768): 10035,\n",
       " (211, 503, 560, 768): 10609,\n",
       " (211, 503, 685, 768): 10230,\n",
       " (28, 348, 701, 768): 9194,\n",
       " (211, 377, 580, 768): 8446,\n",
       " (211, 503, 560, 769): 9195,\n",
       " (177, 309, 696, 768): 10258,\n",
       " (208, 440, 721, 768): 10347,\n",
       " (6, 377, 571, 780): 2162,\n",
       " (1, 503, 745, 768): 82,\n",
       " (15, 400, 623, 768): 7360,\n",
       " (92, 354, 685, 768): 7756,\n",
       " (158, 503, 641, 768): 6000,\n",
       " (2, 503, 751, 768): 7295,\n",
       " (32, 278, 765, 768): 6309,\n",
       " (233, 413, 559, 768): 437,\n",
       " (140, 260, 566, 768): 7656,\n",
       " (150, 316, 585, 768): 6789,\n",
       " (211, 290, 539, 768): 4505,\n",
       " (95, 476, 701, 768): 2100,\n",
       " (62, 510, 559, 768): 4748,\n",
       " (22, 272, 716, 768): 4958,\n",
       " (44, 274, 534, 768): 331,\n",
       " (138, 510, 644, 768): 9417,\n",
       " (211, 348, 720, 768): 332,\n",
       " (2, 459, 559, 768): 2035,\n",
       " (2, 470, 721, 768): 2017,\n",
       " (138, 390, 718, 768): 3474,\n",
       " (138, 269, 583, 768): 4061,\n",
       " (80, 479, 664, 768): 8826,\n",
       " (22, 345, 686, 768): 7229,\n",
       " (54, 407, 571, 768): 10762,\n",
       " (83, 371, 559, 768): 10798,\n",
       " (6, 377, 571, 781): 1416,\n",
       " (1, 371, 751, 768): 5015,\n",
       " (1, 371, 567, 769): 5016,\n",
       " (32, 348, 745, 768): 7772,\n",
       " (121, 309, 621, 768): 9914,\n",
       " (80, 479, 664, 769): 5605,\n",
       " (37, 312, 560, 768): 5145,\n",
       " (126, 503, 685, 768): 6050,\n",
       " (169, 479, 646, 768): 9778,\n",
       " (11, 278, 628, 768): 11149,\n",
       " (211, 459, 609, 768): 15,\n",
       " (211, 459, 609, 769): 9562,\n",
       " (177, 290, 715, 768): 5490,\n",
       " (177, 290, 621, 768): 5523,\n",
       " (28, 394, 746, 768): 8880,\n",
       " (13, 261, 696, 768): 11453,\n",
       " (6, 377, 571, 782): 271,\n",
       " (15, 377, 539, 768): 10085,\n",
       " (15, 400, 697, 768): 4668,\n",
       " (15, 460, 539, 768): 277,\n",
       " (2, 470, 571, 768): 8684,\n",
       " (224, 346, 571, 768): 9369,\n",
       " (6, 377, 571, 783): 10598,\n",
       " (6, 377, 571, 784): 3232,\n",
       " (83, 378, 580, 768): 8583,\n",
       " (84, 400, 715, 768): 10856,\n",
       " (28, 313, 715, 768): 6727,\n",
       " (140, 324, 641, 768): 8569,\n",
       " (160, 421, 547, 768): 10996,\n",
       " (138, 300, 718, 768): 10511,\n",
       " (37, 360, 560, 768): 11138,\n",
       " (75, 290, 718, 768): 5040,\n",
       " (183, 372, 571, 768): 1824,\n",
       " (27, 340, 640, 768): 8990,\n",
       " (177, 470, 621, 768): 4733,\n",
       " (211, 348, 720, 769): 3404,\n",
       " (44, 348, 753, 768): 10003,\n",
       " (177, 460, 751, 768): 10942,\n",
       " (219, 402, 609, 768): 6903,\n",
       " (95, 503, 628, 768): 6993,\n",
       " (211, 470, 564, 768): 8471,\n",
       " (6, 377, 571, 785): 6944,\n",
       " (140, 396, 646, 768): 2847,\n",
       " (33, 280, 734, 768): 6251,\n",
       " (33, 392, 743, 768): 688,\n",
       " (95, 377, 745, 768): 7117,\n",
       " (233, 413, 628, 768): 9031,\n",
       " (126, 378, 745, 768): 10545,\n",
       " (126, 394, 658, 768): 9947,\n",
       " (44, 316, 559, 768): 185,\n",
       " (2, 348, 745, 768): 4563,\n",
       " (2, 408, 628, 768): 8122,\n",
       " (1, 500, 675, 768): 9229,\n",
       " (211, 377, 745, 768): 1880,\n",
       " (1, 378, 609, 769): 5194,\n",
       " (236, 346, 548, 768): 360,\n",
       " (2, 425, 539, 768): 9534,\n",
       " (121, 296, 560, 768): 3803,\n",
       " (239, 378, 657, 768): 6311,\n",
       " (158, 387, 726, 768): 6334,\n",
       " (75, 400, 715, 768): 2245,\n",
       " (186, 366, 678, 768): 8243,\n",
       " (254, 414, 539, 768): 10778,\n",
       " (37, 476, 718, 768): 10239,\n",
       " (0, 402, 609, 768): 6678,\n",
       " (121, 408, 512, 768): 7248,\n",
       " (224, 459, 724, 768): 3143,\n",
       " (236, 280, 548, 768): 407,\n",
       " (121, 272, 628, 768): 9401,\n",
       " (120, 500, 567, 768): 2309,\n",
       " (224, 470, 621, 768): 642,\n",
       " (211, 290, 539, 769): 4065,\n",
       " (2, 378, 745, 768): 6801,\n",
       " (117, 332, 521, 768): 5306,\n",
       " (117, 482, 528, 768): 24,\n",
       " (222, 286, 648, 768): 926,\n",
       " (154, 313, 580, 768): 10454,\n",
       " (6, 377, 571, 786): 467,\n",
       " (173, 387, 564, 768): 1990,\n",
       " (6, 377, 571, 787): 4653,\n",
       " (114, 430, 521, 768): 3878,\n",
       " (114, 372, 567, 768): 8485,\n",
       " (140, 466, 686, 768): 8218,\n",
       " (140, 400, 579, 768): 4310,\n",
       " (22, 306, 521, 768): 491,\n",
       " (211, 290, 571, 768): 5511,\n",
       " (177, 503, 745, 768): 6116,\n",
       " (177, 305, 548, 768): 5967,\n",
       " (75, 290, 521, 768): 2270,\n",
       " (211, 290, 539, 770): 6434,\n",
       " (75, 290, 745, 768): 6003,\n",
       " (2, 425, 559, 768): 4483,\n",
       " (211, 290, 539, 771): 8559,\n",
       " (254, 324, 715, 768): 8730,\n",
       " (126, 299, 547, 768): 9460,\n",
       " (75, 290, 685, 768): 9456,\n",
       " (211, 290, 539, 772): 9592,\n",
       " (173, 500, 688, 768): 8644,\n",
       " (32, 312, 629, 768): 9884,\n",
       " (245, 323, 556, 768): 9253,\n",
       " (70, 346, 628, 768): 2617,\n",
       " (224, 307, 721, 768): 2618,\n",
       " (75, 500, 521, 768): 9250,\n",
       " (253, 272, 688, 768): 2619,\n",
       " (14, 478, 645, 768): 9251,\n",
       " (1, 274, 721, 768): 10698,\n",
       " (121, 495, 561, 768): 10702,\n",
       " (18, 265, 746, 768): 10571,\n",
       " (173, 503, 715, 768): 4439,\n",
       " (173, 434, 571, 768): 3714,\n",
       " (217, 466, 658, 768): 10374,\n",
       " (236, 289, 621, 768): 10670,\n",
       " (121, 390, 561, 768): 8300,\n",
       " (236, 260, 753, 768): 2620,\n",
       " (62, 378, 716, 768): 5091,\n",
       " (253, 320, 738, 768): 10373,\n",
       " (75, 371, 534, 768): 6389,\n",
       " (220, 264, 766, 768): 10808,\n",
       " (195, 286, 721, 768): 5092,\n",
       " (120, 341, 521, 768): 5090,\n",
       " (126, 396, 721, 768): 5093,\n",
       " (126, 396, 578, 768): 10757,\n",
       " (1, 407, 548, 768): 11811,\n",
       " (211, 470, 745, 769): 2054,\n",
       " (1, 290, 521, 768): 2311,\n",
       " (211, 290, 539, 773): 11766,\n",
       " (95, 470, 560, 768): 11778,\n",
       " (211, 470, 559, 768): 11735,\n",
       " (211, 470, 559, 769): 6435,\n",
       " (37, 348, 539, 768): 11764,\n",
       " (211, 470, 560, 769): 12026,\n",
       " (2, 503, 539, 768): 11903,\n",
       " (1, 290, 621, 768): 2733,\n",
       " (211, 500, 628, 768): 11820,\n",
       " (2, 313, 571, 768): 2925,\n",
       " (6, 377, 571, 788): 7818,\n",
       " (211, 470, 559, 770): 3114,\n",
       " (126, 431, 721, 768): 3704,\n",
       " (211, 460, 751, 768): 4697,\n",
       " (53, 296, 718, 768): 9585,\n",
       " (6, 377, 571, 789): 6849,\n",
       " (74, 445, 716, 768): 9590,\n",
       " (1, 346, 751, 768): 10561,\n",
       " (211, 470, 559, 771): 10528,\n",
       " (173, 314, 677, 768): 3715,\n",
       " (114, 393, 766, 768): 10558,\n",
       " (240, 278, 719, 768): 10690,\n",
       " (114, 302, 688, 768): 11904,\n",
       " (22, 394, 766, 768): 10917,\n",
       " (28, 431, 522, 768): 10727,\n",
       " (2, 470, 521, 768): 11615,\n",
       " (120, 290, 628, 768): 3604,\n",
       " (200, 314, 724, 768): 7136,\n",
       " (224, 388, 677, 768): 5325,\n",
       " (211, 470, 745, 770): 9784,\n",
       " (211, 470, 534, 768): 5394,\n",
       " (28, 431, 522, 769): 5186,\n",
       " (173, 314, 726, 768): 6495,\n",
       " (28, 316, 609, 768): 7724,\n",
       " (126, 394, 646, 768): 7652,\n",
       " (75, 378, 751, 769): 5802,\n",
       " (2, 470, 522, 768): 5326,\n",
       " (2, 425, 623, 768): 6032,\n",
       " (95, 302, 715, 768): 7532,\n",
       " (211, 470, 560, 770): 10004,\n",
       " (95, 302, 720, 768): 9735,\n",
       " (0, 277, 589, 768): 5519,\n",
       " (126, 470, 715, 768): 8967,\n",
       " (219, 440, 694, 768): 2464,\n",
       " (119, 419, 720, 768): 6858,\n",
       " (37, 302, 715, 768): 6862,\n",
       " (9, 264, 645, 768): 6865,\n",
       " (75, 459, 662, 768): 9387,\n",
       " (229, 411, 715, 768): 5522,\n",
       " (138, 277, 631, 768): 2513,\n",
       " (126, 386, 734, 768): 9688,\n",
       " (211, 503, 751, 768): 6402,\n",
       " (211, 470, 609, 768): 5023,\n",
       " (2, 377, 685, 768): 5977,\n",
       " (126, 378, 609, 768): 6397,\n",
       " (127, 261, 548, 768): 5987,\n",
       " (6, 377, 571, 790): 9034,\n",
       " (37, 374, 641, 768): 10632,\n",
       " (53, 386, 641, 768): 11186,\n",
       " (70, 311, 703, 768): 4549,\n",
       " (205, 365, 615, 768): 9077,\n",
       " (2, 378, 751, 768): 893,\n",
       " (140, 365, 763, 768): 11411,\n",
       " (95, 378, 567, 768): 8635,\n",
       " (28, 302, 628, 768): 3563,\n",
       " (177, 272, 560, 768): 10106,\n",
       " (200, 449, 716, 768): 8150,\n",
       " (126, 354, 560, 768): 5132,\n",
       " (211, 470, 720, 768): 10435,\n",
       " (211, 290, 685, 768): 11249,\n",
       " (126, 290, 652, 768): 2038,\n",
       " (28, 491, 609, 768): 7130,\n",
       " (211, 290, 539, 774): 8227,\n",
       " (75, 346, 745, 768): 6461,\n",
       " (193, 500, 644, 768): 2308,\n",
       " (144, 388, 571, 768): 327,\n",
       " (6, 377, 571, 791): 11082,\n",
       " (236, 286, 641, 768): 12001,\n",
       " (236, 306, 719, 768): 1904,\n",
       " (1, 377, 695, 768): 6360,\n",
       " (173, 377, 573, 768): 5170,\n",
       " (168, 434, 724, 768): 10464,\n",
       " (156, 463, 512, 768): 227,\n",
       " (32, 278, 726, 768): 6804,\n",
       " (177, 378, 675, 768): 3897,\n",
       " (211, 377, 539, 768): 537,\n",
       " (211, 470, 721, 768): 6807,\n",
       " (28, 423, 697, 768): 11651,\n",
       " (1, 459, 720, 768): 11939,\n",
       " (2, 459, 566, 768): 11874,\n",
       " (6, 377, 571, 792): 4880,\n",
       " (219, 316, 761, 768): 11791,\n",
       " (28, 263, 539, 768): 8124,\n",
       " (1, 290, 751, 768): 727,\n",
       " (211, 470, 559, 772): 9763,\n",
       " (220, 366, 579, 768): 9620,\n",
       " (168, 332, 720, 768): 12066,\n",
       " (28, 394, 548, 768): 3426,\n",
       " (2, 503, 573, 768): 8851,\n",
       " (135, 460, 753, 768): 9110,\n",
       " (135, 346, 721, 768): 9853,\n",
       " (219, 346, 567, 768): 9836,\n",
       " (135, 346, 674, 768): 10000,\n",
       " (135, 346, 674, 769): 10134,\n",
       " (193, 377, 621, 768): 8099,\n",
       " (62, 445, 765, 768): 11447,\n",
       " (135, 413, 534, 768): 11141,\n",
       " (219, 316, 677, 768): 10178,\n",
       " (62, 445, 512, 768): 11005,\n",
       " (193, 272, 720, 768): 2045,\n",
       " (62, 295, 631, 768): 5578,\n",
       " (219, 377, 606, 768): 10223,\n",
       " (127, 450, 676, 768): 11600,\n",
       " (135, 367, 640, 768): 9279,\n",
       " (135, 313, 559, 768): 917,\n",
       " (127, 289, 512, 768): 1492,\n",
       " (177, 470, 641, 768): 3081,\n",
       " (135, 328, 648, 768): 10746,\n",
       " (239, 318, 745, 768): 4263,\n",
       " (84, 318, 541, 768): 6388,\n",
       " (193, 348, 720, 768): 9083,\n",
       " (178, 318, 580, 768): 9088,\n",
       " (220, 348, 644, 768): 10335,\n",
       " (2, 470, 677, 768): 10356,\n",
       " (28, 459, 631, 768): 55,\n",
       " (1, 434, 751, 768): 3163,\n",
       " (126, 459, 644, 768): 10865,\n",
       " (28, 272, 615, 768): 8562,\n",
       " (1, 478, 521, 768): 3433,\n",
       " (150, 371, 696, 768): 3928,\n",
       " (150, 388, 761, 768): 1254,\n",
       " (126, 378, 745, 769): 3071,\n",
       " (6, 377, 571, 793): 10667,\n",
       " (239, 316, 718, 768): 8807,\n",
       " (75, 470, 751, 768): 4661,\n",
       " (154, 295, 750, 768): 6946,\n",
       " (6, 377, 571, 794): 1854,\n",
       " (211, 503, 564, 768): 9244,\n",
       " (168, 290, 715, 768): 3013,\n",
       " (219, 348, 621, 768): 1690,\n",
       " (75, 371, 751, 768): 2577,\n",
       " (1, 400, 751, 768): 2423,\n",
       " (211, 290, 720, 768): 53,\n",
       " (173, 296, 721, 768): 10428,\n",
       " (2, 408, 675, 768): 4712,\n",
       " (211, 470, 548, 768): 2933,\n",
       " (128, 360, 763, 768): 11554,\n",
       " (128, 394, 688, 768): 1734,\n",
       " (220, 491, 724, 768): 8035,\n",
       " (113, 312, 701, 768): 10436,\n",
       " (128, 360, 560, 768): 595,\n",
       " (2, 408, 677, 768): 6142,\n",
       " (2, 408, 745, 768): 4180,\n",
       " (44, 398, 721, 768): 485,\n",
       " (44, 478, 645, 768): 3663,\n",
       " (113, 365, 701, 768): 10995,\n",
       " (221, 436, 685, 768): 11060,\n",
       " (26, 312, 646, 768): 772,\n",
       " (126, 377, 564, 768): 1930,\n",
       " (243, 293, 564, 768): 2639,\n",
       " (117, 378, 521, 768): 2501,\n",
       " (75, 478, 539, 768): 3927,\n",
       " (177, 348, 670, 768): 7218,\n",
       " (211, 290, 539, 775): 9568,\n",
       " (123, 390, 721, 768): 341,\n",
       " (1, 503, 718, 768): 8647,\n",
       " (211, 302, 644, 768): 5257,\n",
       " (2, 470, 521, 769): 8279,\n",
       " (75, 313, 685, 768): 2440,\n",
       " (211, 470, 720, 769): 6086,\n",
       " (211, 470, 609, 769): 4169,\n",
       " (53, 289, 701, 768): 8234,\n",
       " (95, 476, 701, 769): 6328,\n",
       " (2, 503, 559, 768): 3358,\n",
       " (95, 274, 560, 768): 7419,\n",
       " (128, 263, 560, 768): 3421,\n",
       " (120, 470, 716, 768): 6953,\n",
       " (211, 425, 718, 768): 8575,\n",
       " (28, 323, 720, 768): 9600,\n",
       " (28, 475, 623, 768): 5106,\n",
       " (121, 332, 606, 768): 5107,\n",
       " (144, 364, 522, 768): 5188,\n",
       " (53, 372, 559, 768): 6484,\n",
       " (53, 372, 609, 768): 7317,\n",
       " (160, 460, 694, 768): 7614,\n",
       " (126, 460, 661, 768): 5533,\n",
       " (186, 495, 747, 768): 6458,\n",
       " (186, 387, 522, 768): 2175,\n",
       " (211, 290, 539, 776): 7479,\n",
       " (173, 371, 564, 768): 3113,\n",
       " (177, 302, 720, 768): 8416,\n",
       " (2, 503, 559, 769): 7972,\n",
       " (6, 377, 571, 795): 10055,\n",
       " (211, 378, 720, 769): 9292,\n",
       " (6, 377, 571, 796): 5858,\n",
       " (6, 377, 571, 797): 4993,\n",
       " (156, 405, 652, 768): 4665,\n",
       " (80, 302, 664, 768): 3463,\n",
       " (95, 377, 623, 768): 6523,\n",
       " (211, 313, 559, 768): 3018,\n",
       " (211, 503, 751, 769): 253,\n",
       " (1, 400, 719, 768): 8737,\n",
       " (2, 503, 559, 770): 1544,\n",
       " (28, 409, 560, 768): 555,\n",
       " (74, 445, 644, 768): 2549,\n",
       " (156, 461, 697, 768): 10416,\n",
       " (173, 476, 567, 768): 8346,\n",
       " (27, 425, 560, 768): 4672,\n",
       " (224, 318, 718, 768): 8738,\n",
       " (193, 296, 623, 768): 10731,\n",
       " (32, 500, 715, 768): 10827,\n",
       " (37, 290, 580, 768): 11248,\n",
       " (211, 296, 751, 768): 1468,\n",
       " (126, 348, 560, 768): 3602,\n",
       " (95, 408, 753, 768): 4955,\n",
       " (75, 302, 623, 768): 5843,\n",
       " (72, 460, 583, 768): 6486,\n",
       " (121, 503, 677, 768): 3594,\n",
       " (53, 278, 745, 768): 6462,\n",
       " (22, 449, 539, 768): 6465,\n",
       " (6, 377, 571, 798): 6487,\n",
       " (254, 419, 716, 768): 6973,\n",
       " (229, 345, 765, 768): 6616,\n",
       " (9, 436, 579, 768): 7459,\n",
       " (135, 386, 571, 768): 6403,\n",
       " (114, 434, 662, 768): 11923,\n",
       " (200, 260, 677, 768): 11894,\n",
       " (185, 436, 661, 768): 10839,\n",
       " (185, 409, 747, 768): 868,\n",
       " (211, 302, 564, 768): 3208,\n",
       " (222, 312, 718, 768): 739,\n",
       " (254, 424, 521, 768): 3848,\n",
       " (156, 371, 644, 768): 5293,\n",
       " (121, 296, 685, 768): 7512,\n",
       " (2, 408, 522, 768): 4023,\n",
       " (53, 500, 547, 768): 2898,\n",
       " (2, 408, 522, 769): 6149,\n",
       " (120, 302, 720, 768): 5197,\n",
       " (18, 423, 716, 768): 1915,\n",
       " (150, 265, 541, 768): 6700,\n",
       " (13, 374, 629, 768): 2722,\n",
       " (35, 442, 716, 768): 7003,\n",
       " (114, 495, 695, 768): 6208,\n",
       " (28, 261, 522, 768): 7937,\n",
       " (2, 425, 559, 769): 199,\n",
       " (22, 306, 571, 768): 1464,\n",
       " (114, 394, 715, 768): 5384,\n",
       " (121, 431, 766, 768): 3209,\n",
       " (62, 388, 623, 768): 330,\n",
       " (245, 346, 753, 768): 5535,\n",
       " (2, 371, 750, 768): 5954,\n",
       " (121, 290, 653, 768): 2022,\n",
       " (166, 269, 594, 768): 4056,\n",
       " (6, 377, 571, 799): 4187,\n",
       " (75, 290, 745, 769): 6869,\n",
       " (166, 269, 738, 768): 1768,\n",
       " (121, 346, 662, 768): 7768,\n",
       " (121, 470, 560, 768): 2708,\n",
       " (75, 470, 574, 768): 7001,\n",
       " (236, 312, 724, 768): 770,\n",
       " (205, 274, 539, 768): 3036,\n",
       " (22, 400, 567, 768): 1802,\n",
       " (75, 260, 522, 768): 720,\n",
       " (173, 296, 573, 768): 6359,\n",
       " (75, 503, 745, 768): 9633,\n",
       " (224, 341, 522, 768): 7565,\n",
       " (73, 510, 726, 768): 6673,\n",
       " (128, 302, 628, 768): 1601,\n",
       " (53, 277, 719, 768): 680,\n",
       " (53, 511, 701, 768): 4475,\n",
       " (114, 386, 697, 768): 6965,\n",
       " (193, 332, 701, 768): 8784,\n",
       " (168, 290, 753, 768): 8785,\n",
       " (138, 277, 629, 768): 844,\n",
       " (75, 431, 559, 768): 1507,\n",
       " (173, 431, 720, 768): 1508,\n",
       " (42, 348, 573, 768): 3066,\n",
       " (80, 306, 567, 768): 9035,\n",
       " (200, 460, 621, 768): 9584,\n",
       " (53, 274, 716, 768): 3182,\n",
       " (53, 274, 559, 768): 3348,\n",
       " (53, 323, 761, 768): 9587,\n",
       " (6, 377, 571, 800): 9595,\n",
       " (236, 311, 746, 768): 10699,\n",
       " (120, 503, 522, 768): 10812,\n",
       " (37, 261, 559, 768): 9086,\n",
       " (53, 393, 701, 768): 10896,\n",
       " (150, 407, 580, 768): 10669,\n",
       " (62, 489, 765, 768): 10164,\n",
       " (2, 388, 559, 768): 8977,\n",
       " (211, 470, 609, 770): 10752,\n",
       " (211, 296, 522, 769): 11054,\n",
       " (2, 425, 674, 768): 10753,\n",
       " (211, 290, 539, 777): 8976,\n",
       " (95, 503, 571, 768): 1291,\n",
       " (2, 470, 560, 768): 11771,\n",
       " (1, 400, 751, 769): 11765,\n",
       " (2, 503, 720, 768): 11802,\n",
       " (32, 494, 641, 768): 11803,\n",
       " (211, 313, 559, 769): 3549,\n",
       " (75, 503, 745, 769): 9561,\n",
       " (1, 503, 677, 768): 6433,\n",
       " (95, 302, 715, 769): 11780,\n",
       " (1, 290, 697, 768): 323,\n",
       " (1, 407, 573, 768): 10950,\n",
       " (75, 470, 628, 768): 2472,\n",
       " (211, 378, 522, 768): 1927,\n",
       " (205, 328, 512, 768): 103,\n",
       " (6, 377, 571, 801): 263,\n",
       " (171, 290, 534, 768): 553,\n",
       " (211, 470, 539, 768): 3773,\n",
       " (224, 503, 745, 768): 7847,\n",
       " (37, 482, 716, 768): 8538,\n",
       " (220, 445, 573, 768): 8954,\n",
       " (135, 390, 548, 768): 7132,\n",
       " (62, 291, 560, 768): 4996,\n",
       " (135, 360, 695, 768): 10565,\n",
       " (70, 305, 574, 768): 6210,\n",
       " (211, 460, 745, 768): 3279,\n",
       " (169, 499, 751, 768): 909,\n",
       " (193, 348, 750, 768): 1121,\n",
       " (177, 491, 564, 768): 5243,\n",
       " (0, 303, 657, 768): 9084,\n",
       " (211, 290, 720, 769): 8026,\n",
       " (126, 478, 560, 768): 8153,\n",
       " (70, 309, 646, 768): 2972,\n",
       " (219, 424, 589, 768): 9380,\n",
       " (211, 470, 609, 771): 1522,\n",
       " (53, 470, 761, 768): 1827,\n",
       " (114, 409, 720, 768): 717,\n",
       " (119, 449, 696, 768): 11452,\n",
       " (227, 341, 761, 768): 4898,\n",
       " (121, 302, 761, 768): 6735,\n",
       " (72, 460, 645, 768): 1875,\n",
       " (166, 263, 573, 768): 7210,\n",
       " (227, 414, 655, 768): 8889,\n",
       " (26, 510, 559, 768): 8044,\n",
       " (18, 414, 750, 768): 9196,\n",
       " (205, 346, 655, 768): 9952,\n",
       " (205, 346, 696, 768): 9189,\n",
       " (62, 299, 526, 768): 4367,\n",
       " (121, 299, 721, 768): 10016,\n",
       " (156, 286, 685, 768): 3541,\n",
       " (121, 290, 653, 769): 3654,\n",
       " (120, 387, 522, 768): 9266,\n",
       " (177, 311, 750, 768): 4868,\n",
       " (75, 290, 621, 768): 11403,\n",
       " (37, 489, 669, 768): 10385,\n",
       " (75, 265, 685, 768): 8808,\n",
       " (1, 302, 715, 768): 3050,\n",
       " (37, 313, 609, 768): 9883,\n",
       " (1, 459, 716, 768): 11664,\n",
       " (1, 388, 534, 768): 12033,\n",
       " (18, 494, 641, 768): 1577,\n",
       " (177, 470, 560, 768): 3780,\n",
       " (1, 470, 560, 768): 3167,\n",
       " (126, 274, 559, 768): 4441,\n",
       " (37, 277, 609, 768): 6770,\n",
       " (156, 277, 589, 768): 4978,\n",
       " (2, 459, 648, 768): 6743,\n",
       " (140, 323, 560, 768): 3362,\n",
       " (75, 511, 560, 768): 8380,\n",
       " (26, 459, 670, 768): 3763,\n",
       " (32, 394, 751, 768): 4024,\n",
       " (0, 499, 688, 768): 9309,\n",
       " (37, 499, 644, 768): 10282,\n",
       " (37, 394, 522, 768): 10560,\n",
       " (37, 274, 688, 768): 11767,\n",
       " (37, 500, 628, 768): 10506,\n",
       " (37, 408, 609, 768): 2662,\n",
       " (224, 313, 522, 768): 2689,\n",
       " (224, 377, 661, 768): 9728,\n",
       " (75, 470, 751, 769): 5007,\n",
       " (37, 263, 605, 768): 9477,\n",
       " (75, 302, 628, 768): 9468,\n",
       " (37, 314, 628, 768): 248,\n",
       " (224, 377, 661, 769): 2884,\n",
       " (224, 377, 661, 770): 5677,\n",
       " (37, 314, 548, 768): 163,\n",
       " (224, 503, 761, 768): 10689,\n",
       " (37, 503, 745, 768): 11710,\n",
       " (37, 371, 559, 768): 5245,\n",
       " (37, 274, 560, 768): 10693,\n",
       " (75, 408, 628, 768): 8820,\n",
       " (186, 394, 641, 768): 1593,\n",
       " (211, 470, 560, 771): 11370,\n",
       " (211, 316, 745, 768): 8092,\n",
       " (114, 436, 583, 768): 11375,\n",
       " (6, 377, 571, 802): 10512,\n",
       " (222, 333, 641, 768): 2468,\n",
       " (6, 377, 571, 803): 2987,\n",
       " (173, 302, 662, 768): 4911,\n",
       " (120, 427, 539, 768): 10291,\n",
       " (114, 333, 750, 768): 6981,\n",
       " (211, 470, 559, 773): 4910,\n",
       " (6, 377, 571, 804): 3691,\n",
       " (211, 470, 609, 772): 3989,\n",
       " (156, 449, 580, 768): 2032,\n",
       " (224, 460, 720, 768): 47,\n",
       " (173, 500, 521, 768): 8313,\n",
       " (173, 431, 761, 768): 3252,\n",
       " (1, 286, 701, 768): 7215,\n",
       " (1, 302, 548, 768): 7523,\n",
       " (201, 460, 559, 768): 2037,\n",
       " (32, 289, 631, 768): 4914,\n",
       " (18, 409, 621, 768): 5352,\n",
       " (62, 346, 658, 768): 5030,\n",
       " (32, 348, 697, 768): 5564,\n",
       " (126, 408, 715, 768): 10740,\n",
       " (156, 479, 765, 768): 6302,\n",
       " (53, 400, 701, 768): 1118,\n",
       " (186, 367, 525, 768): 3990,\n",
       " (1, 302, 715, 769): 546,\n",
       " (173, 372, 573, 768): 1085,\n",
       " (6, 377, 571, 805): 5096,\n",
       " (18, 402, 560, 768): 837,\n",
       " (73, 510, 649, 768): 7660,\n",
       " (156, 408, 765, 768): 5168,\n",
       " (140, 460, 548, 768): 10442,\n",
       " (177, 313, 512, 768): 2573,\n",
       " (177, 302, 715, 769): 6765,\n",
       " (135, 313, 621, 768): 818,\n",
       " (211, 470, 560, 772): 6132,\n",
       " (211, 313, 560, 768): 1057,\n",
       " (183, 408, 571, 768): 281,\n",
       " (6, 377, 571, 806): 4916,\n",
       " (37, 305, 685, 768): 3769,\n",
       " (173, 431, 564, 768): 11928,\n",
       " (126, 407, 716, 768): 6733,\n",
       " (228, 369, 548, 768): 1354,\n",
       " (128, 388, 751, 768): 2811,\n",
       " (28, 408, 675, 768): 71,\n",
       " (139, 295, 751, 768): 1352,\n",
       " (211, 470, 761, 768): 10382,\n",
       " (2, 332, 685, 768): 1262,\n",
       " (139, 295, 751, 769): 2558,\n",
       " (233, 413, 560, 768): 11216,\n",
       " (211, 470, 559, 774): 11711,\n",
       " (74, 409, 695, 768): 11220,\n",
       " (95, 269, 525, 768): 11770,\n",
       " (211, 296, 522, 770): 11712,\n",
       " (2, 425, 677, 768): 11708,\n",
       " (32, 299, 753, 768): 4418,\n",
       " (6, 377, 571, 807): 1697,\n",
       " (37, 261, 685, 768): 3945,\n",
       " (211, 470, 655, 768): 7007,\n",
       " (75, 261, 595, 768): 4618,\n",
       " (211, 286, 685, 768): 849,\n",
       " (157, 431, 606, 768): 899,\n",
       " (158, 369, 644, 768): 2009,\n",
       " (75, 378, 522, 768): 6452,\n",
       " (177, 318, 522, 768): 3200,\n",
       " (44, 272, 652, 768): 2374,\n",
       " (169, 390, 623, 768): 6768,\n",
       " (98, 489, 539, 768): 11385,\n",
       " (75, 500, 648, 768): 8786,\n",
       " (2, 503, 719, 768): 268,\n",
       " (211, 290, 539, 778): 3743,\n",
       " (2, 346, 716, 768): 9847,\n",
       " (224, 274, 747, 768): 10797,\n",
       " (186, 300, 571, 768): 7477,\n",
       " (2, 408, 745, 769): 9404,\n",
       " (135, 295, 645, 768): 721,\n",
       " (75, 296, 745, 768): 4032,\n",
       " (53, 261, 574, 768): 2377,\n",
       " (6, 377, 571, 808): 7366,\n",
       " (177, 290, 641, 768): 2608,\n",
       " (37, 425, 644, 768): 2764,\n",
       " (224, 503, 548, 768): 3758,\n",
       " (224, 377, 522, 768): 7181,\n",
       " (224, 378, 522, 768): 8472,\n",
       " (224, 377, 685, 768): 4654,\n",
       " (220, 463, 655, 768): 8486,\n",
       " (75, 302, 564, 768): 4985,\n",
       " (254, 450, 751, 768): 3504,\n",
       " (97, 274, 621, 768): 8129,\n",
       " (239, 358, 534, 768): 7535,\n",
       " (97, 510, 615, 768): 10245,\n",
       " (97, 323, 697, 768): 1001,\n",
       " (214, 502, 761, 768): 9921,\n",
       " (211, 470, 560, 773): 5362,\n",
       " (126, 316, 753, 768): 5635,\n",
       " (167, 346, 571, 768): 2983,\n",
       " (168, 290, 761, 768): 5697,\n",
       " (219, 372, 609, 768): 11784,\n",
       " (84, 503, 560, 768): 7172,\n",
       " (169, 369, 751, 768): 10481,\n",
       " (168, 263, 677, 768): 429,\n",
       " (187, 265, 548, 768): 6138,\n",
       " (187, 482, 585, 768): 5413,\n",
       " (168, 409, 580, 768): 7096,\n",
       " (187, 289, 688, 768): 159,\n",
       " (168, 265, 585, 768): 4336,\n",
       " (187, 434, 715, 768): 1585,\n",
       " (187, 387, 716, 768): 4189,\n",
       " (219, 407, 580, 768): 5501,\n",
       " (171, 354, 745, 768): 1912,\n",
       " (84, 323, 522, 768): 2076,\n",
       " (168, 393, 745, 768): 3699,\n",
       " (168, 312, 609, 768): 10570,\n",
       " (168, 434, 745, 768): 3195,\n",
       " (168, 290, 715, 769): 5783,\n",
       " (84, 434, 621, 768): 4271,\n",
       " (171, 409, 746, 768): 5831,\n",
       " (171, 332, 609, 768): 483,\n",
       " (171, 495, 579, 768): 7095,\n",
       " (6, 377, 571, 809): 5878,\n",
       " (219, 316, 560, 769): 3832,\n",
       " (187, 434, 522, 768): 2200,\n",
       " (168, 407, 719, 768): 6613,\n",
       " (187, 434, 595, 768): 9392,\n",
       " (84, 312, 609, 768): 2028,\n",
       " (193, 503, 762, 768): 1274,\n",
       " (219, 500, 641, 768): 9966,\n",
       " (193, 309, 641, 768): 5881,\n",
       " (168, 290, 753, 769): 10573,\n",
       " (168, 290, 753, 770): 1213,\n",
       " (84, 408, 753, 768): 3596,\n",
       " (219, 510, 675, 768): 7689,\n",
       " (84, 312, 561, 768): 6906,\n",
       " (193, 396, 724, 768): 10696,\n",
       " (37, 377, 750, 768): 4691,\n",
       " (95, 394, 661, 768): 1294,\n",
       " (95, 377, 559, 768): 6528,\n",
       " (138, 299, 685, 768): 10384,\n",
       " (138, 416, 674, 768): 9360,\n",
       " (6, 377, 571, 810): 11102,\n",
       " (2, 305, 631, 768): 847,\n",
       " (75, 378, 719, 768): 11312,\n",
       " (95, 371, 560, 768): 11311,\n",
       " (6, 377, 571, 811): 5200,\n",
       " (95, 371, 628, 768): 4450,\n",
       " (236, 430, 534, 768): 2506,\n",
       " (75, 503, 628, 768): 4510,\n",
       " (224, 290, 560, 768): 11922,\n",
       " (95, 323, 688, 768): 487,\n",
       " (75, 394, 609, 768): 8250,\n",
       " (123, 460, 641, 768): 10133,\n",
       " (98, 377, 628, 768): 8958,\n",
       " (75, 296, 559, 768): 326,\n",
       " (6, 377, 571, 812): 10616,\n",
       " (37, 377, 677, 768): 7930,\n",
       " (211, 408, 753, 768): 9999,\n",
       " (254, 290, 719, 768): 11271,\n",
       " (254, 414, 646, 768): 2547,\n",
       " (2, 470, 628, 768): 8074,\n",
       " (113, 365, 589, 768): 10706,\n",
       " (2, 290, 715, 768): 7911,\n",
       " (138, 350, 761, 768): 10566,\n",
       " (168, 313, 655, 768): 8556,\n",
       " (53, 365, 520, 768): 10538,\n",
       " (73, 449, 515, 768): 267,\n",
       " (185, 328, 715, 768): 7807,\n",
       " (127, 274, 718, 768): 7135,\n",
       " (185, 386, 747, 768): 4016,\n",
       " (98, 378, 548, 768): 2673,\n",
       " (98, 400, 695, 768): 7881,\n",
       " (6, 377, 571, 813): 7880,\n",
       " (211, 503, 720, 768): 11512,\n",
       " (221, 360, 583, 768): 6237,\n",
       " (211, 470, 522, 769): 10108,\n",
       " (211, 470, 534, 769): 9990,\n",
       " (173, 408, 675, 768): 404,\n",
       " (173, 503, 745, 768): 2128,\n",
       " (2, 503, 512, 768): 6366,\n",
       " (42, 459, 662, 768): 7991,\n",
       " (6, 377, 571, 814): 8649,\n",
       " (156, 376, 525, 768): 11300,\n",
       " (156, 277, 629, 768): 11287,\n",
       " (185, 419, 664, 768): 8744,\n",
       " (6, 377, 571, 815): 7743,\n",
       " (98, 289, 718, 768): 11345,\n",
       " (28, 459, 560, 768): 11292,\n",
       " (2, 425, 539, 769): 4268,\n",
       " (74, 460, 571, 768): 11338,\n",
       " (1, 470, 560, 769): 5343,\n",
       " (150, 500, 629, 768): 2720,\n",
       " (75, 459, 688, 768): 1240,\n",
       " (54, 369, 750, 768): 5528,\n",
       " (95, 377, 715, 768): 6048,\n",
       " (37, 296, 621, 768): 1027,\n",
       " (32, 400, 534, 768): 3708,\n",
       " (76, 293, 657, 768): 7700,\n",
       " (75, 470, 522, 768): 5143,\n",
       " (224, 346, 573, 768): 5973,\n",
       " (54, 372, 669, 768): 7226,\n",
       " (224, 346, 573, 769): 5115,\n",
       " (2, 377, 573, 768): 7698,\n",
       " (6, 377, 571, 816): 7701,\n",
       " (121, 470, 559, 768): 8814,\n",
       " (245, 274, 560, 768): 2007,\n",
       " (68, 477, 520, 768): 7717,\n",
       " (121, 290, 628, 768): 7392,\n",
       " (1, 316, 539, 768): 6868,\n",
       " (75, 371, 560, 768): 880,\n",
       " (68, 477, 513, 768): 8711,\n",
       " (68, 427, 548, 768): 5788,\n",
       " (121, 419, 628, 768): 6071,\n",
       " (98, 413, 579, 768): 5790,\n",
       " (121, 371, 574, 768): 2215,\n",
       " (121, 419, 721, 768): 5791,\n",
       " (75, 503, 751, 768): 220,\n",
       " (28, 460, 719, 768): 7448,\n",
       " (236, 394, 670, 768): 4435,\n",
       " (193, 332, 750, 768): 5786,\n",
       " (126, 407, 659, 768): 5792,\n",
       " (95, 431, 580, 768): 3002,\n",
       " (243, 293, 513, 768): 5794,\n",
       " (95, 312, 609, 768): 2605,\n",
       " (6, 377, 571, 817): 3222,\n",
       " (144, 390, 719, 768): 7795,\n",
       " (9, 264, 645, 769): 6916,\n",
       " (156, 348, 589, 768): 2360,\n",
       " (177, 274, 580, 768): 7538,\n",
       " (186, 302, 649, 768): 2793,\n",
       " (178, 410, 721, 768): 6760,\n",
       " (75, 503, 571, 768): 8625,\n",
       " (2, 290, 539, 768): 8766,\n",
       " (2, 378, 621, 768): 8139,\n",
       " (233, 387, 756, 768): 9203,\n",
       " (233, 413, 688, 768): 2348,\n",
       " (156, 358, 541, 768): 3713,\n",
       " (37, 503, 720, 768): 8723,\n",
       " (135, 440, 649, 768): 8724,\n",
       " (6, 377, 571, 818): 9202,\n",
       " (84, 323, 522, 769): 6782,\n",
       " (75, 470, 609, 768): 821,\n",
       " (193, 407, 716, 768): 9204,\n",
       " (192, 272, 560, 768): 10646,\n",
       " (185, 489, 657, 768): 828,\n",
       " (32, 348, 745, 769): 1093,\n",
       " (75, 390, 521, 768): 7860,\n",
       " (173, 302, 548, 768): 2249,\n",
       " (6, 377, 571, 819): 11563,\n",
       " (235, 348, 678, 768): 771,\n",
       " (211, 470, 688, 768): 1886,\n",
       " (53, 296, 644, 768): 11244,\n",
       " (1, 377, 609, 768): 6960,\n",
       " (120, 371, 623, 768): 9723,\n",
       " (211, 372, 761, 768): 8198,\n",
       " (128, 274, 751, 768): 2648,\n",
       " (169, 427, 605, 768): 10246,\n",
       " (185, 476, 522, 768): 2337,\n",
       " (128, 470, 628, 768): 3802,\n",
       " (2, 408, 721, 768): 4050,\n",
       " (70, 348, 716, 768): 4052,\n",
       " (70, 400, 573, 768): 5348,\n",
       " (121, 378, 719, 768): 11052,\n",
       " (0, 265, 606, 768): 41,\n",
       " (2, 503, 559, 771): 2149,\n",
       " (211, 377, 559, 768): 3952,\n",
       " (75, 470, 521, 768): 3160,\n",
       " (75, 296, 559, 769): 10315,\n",
       " (211, 332, 720, 768): 9722,\n",
       " (37, 470, 715, 768): 5457,\n",
       " (173, 470, 548, 768): 1530,\n",
       " (75, 470, 609, 769): 9875,\n",
       " (42, 470, 719, 768): 10862,\n",
       " (37, 387, 512, 768): 639,\n",
       " (42, 332, 762, 768): 9998,\n",
       " (75, 470, 548, 768): 8076,\n",
       " (37, 387, 609, 768): 7757,\n",
       " (1, 460, 751, 768): 7424,\n",
       " (75, 277, 761, 768): 5263,\n",
       " (37, 346, 548, 768): 5262,\n",
       " (37, 503, 745, 769): 7141,\n",
       " (37, 387, 621, 768): 6329,\n",
       " (75, 378, 609, 768): 8980,\n",
       " (42, 290, 685, 768): 9876,\n",
       " (28, 377, 631, 768): 3746,\n",
       " (37, 459, 623, 768): 1869,\n",
       " (224, 378, 623, 768): 1989,\n",
       " (1, 302, 685, 768): 9744,\n",
       " (75, 290, 751, 768): 750,\n",
       " (37, 305, 539, 768): 5418,\n",
       " (18, 394, 715, 768): 3844,\n",
       " (75, 378, 751, 770): 2299,\n",
       " (75, 431, 621, 768): 3159,\n",
       " (245, 394, 560, 768): 4369,\n",
       " (11, 394, 674, 768): 9471,\n",
       " (53, 511, 745, 768): 9546,\n",
       " (135, 511, 761, 768): 114,\n",
       " (173, 413, 746, 768): 6192,\n",
       " (144, 277, 685, 768): 7330,\n",
       " (236, 290, 715, 768): 7244,\n",
       " (224, 323, 675, 768): 2339,\n",
       " (6, 377, 571, 820): 6451,\n",
       " (160, 400, 745, 768): 2060,\n",
       " (1, 425, 512, 768): 6683,\n",
       " (173, 377, 521, 768): 7475,\n",
       " (53, 470, 559, 769): 7883,\n",
       " (177, 377, 745, 768): 8108,\n",
       " (2, 378, 560, 768): 9172,\n",
       " (128, 459, 641, 768): 10710,\n",
       " (2, 459, 657, 768): 9577,\n",
       " (28, 421, 747, 768): 11322,\n",
       " (168, 265, 560, 768): 5809,\n",
       " (168, 482, 621, 768): 1586,\n",
       " (219, 316, 560, 770): 4018,\n",
       " (6, 377, 571, 821): 3576,\n",
       " (75, 388, 686, 768): 10503,\n",
       " (75, 470, 548, 769): 8790,\n",
       " (173, 377, 559, 768): 10498,\n",
       " (37, 503, 522, 768): 2319,\n",
       " (173, 377, 721, 768): 254,\n",
       " (75, 460, 559, 768): 408,\n",
       " (74, 332, 738, 768): 7302,\n",
       " (211, 296, 745, 768): 8020,\n",
       " ...}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{tuple(sem_ids): item_id \n",
    "           for item_id, sem_ids in item_sem_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/jovyan/gusak/semantic_seqrec/data/item_sem_id_modified.pkl', 'rb') as f:\n",
    "    index2semid = pickle.load(f)\n",
    "# inv_map = {tuple(sem_ids): item_id for item_id, sem_ids in index2semid.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1706.)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(list(index2semid.values())).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([104, 431, 695, 768])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(index2semid.values())[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "splits",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
