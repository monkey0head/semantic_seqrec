{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../')\n",
    "os.environ['SEQ_SPLITS_DATA_PATH'] = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/shares/SR003.nfs2/volodkevich/seq_splits/av_env_seq_splits/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/jovyan/shares/SR003.nfs2/volodkevich/seq_splits/av_env_seq_splits/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/jovyan/shares/SR003.nfs2/volodkevich/seq_splits/av_env_seq_splits/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/jovyan/shares/SR003.nfs2/volodkevich/seq_splits/av_env_seq_splits/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from runs.train import prepare_data, create_dataloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_627389/3112358418.py:3: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  with initialize(config_path=\"../runs/configs/\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda_visible_devices: 0\n",
      "random_state: 101\n",
      "clearml_project_folder: null\n",
      "clearml_task_name: null\n",
      "use_pretrained_embeddings: false\n",
      "pretrained_embeddings:\n",
      "  add_padding_emb: true\n",
      "  freeze: false\n",
      "use_semantic_ids: false\n",
      "semantic_ids_map_path: data/SemanticID/PATH.pkl\n",
      "split_type: global_timesplit\n",
      "split_subtype: val_by_time\n",
      "quantile: 0.9\n",
      "validation_quantile: 0.9\n",
      "dataset_params:\n",
      "  max_length: 128\n",
      "dataloader:\n",
      "  batch_size: 64\n",
      "  test_batch_size: 64\n",
      "  num_workers: 8\n",
      "  validation_size: 2048\n",
      "seqrec_module:\n",
      "  lr: 0.001\n",
      "  predict_top_k: 10\n",
      "  filter_seen: true\n",
      "trainer_params:\n",
      "  max_epochs: 10\n",
      "  accelerator: gpu\n",
      "patience: 20\n",
      "load_if_possible: false\n",
      "evaluator:\n",
      "  successive_val: false\n",
      "  successive_test: false\n",
      "  successive_test_retrained: false\n",
      "  calc_successive_metrics_val: true\n",
      "  calc_successive_metrics_test: true\n",
      "  calc_successive_metrics_test_retrained: true\n",
      "  successive_replay_metrics: false\n",
      "  metrics:\n",
      "  - NDCG\n",
      "  - HitRate\n",
      "  - MRR\n",
      "  - Coverage\n",
      "  top_k:\n",
      "  - 1\n",
      "  - 5\n",
      "  - 10\n",
      "  - 20\n",
      "  - 50\n",
      "  - 100\n",
      "retrain_with_validation: false\n",
      "save_val_last_predictions: false\n",
      "save_test_last_predictions: false\n",
      "dataset:\n",
      "  name: Beauty\n",
      "  filter_seen: true\n",
      "  column_name:\n",
      "    user_id: user_id\n",
      "    item_id: item_id\n",
      "    timestamp: timestamp\n",
      "    relevance: null\n",
      "  pretrained_embeddings_path: data/embeddings/my_beauty_embs.npy\n",
      "model:\n",
      "  model_class: GPT-2\n",
      "  pdrop: 0.1\n",
      "  model_params:\n",
      "    n_positions: 128\n",
      "    n_embd: 64\n",
      "    n_layer: 2\n",
      "    n_head: 1\n",
      "    embd_pdrop: 0.1\n",
      "    attn_pdrop: 0.1\n",
      "  generation: true\n",
      "  mode: reciprocal_rank_aggregation\n",
      "  generation_params:\n",
      "    num_return_sequences: 1\n",
      "    no_repeat_ngram_size: 1\n",
      "    do_sample: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = OmegaConf.load('../runs/configs/train.yaml')\n",
    "\n",
    "with initialize(config_path=\"../runs/configs/\"):  \n",
    "    config = compose(\n",
    "        config_name=\"train\",      \n",
    "        overrides=[\n",
    "            \"quantile=0.9\",\n",
    "            \"split_subtype=val_by_time\",\n",
    "            \"dataset=Beauty\",\n",
    "            \"model=GPT2\",\n",
    "            \"trainer_params.max_epochs=10\",\n",
    "            \n",
    "        ],\n",
    "        return_hydra_config=False,\n",
    "    )\n",
    "\n",
    "print(OmegaConf.to_yaml(config, resolve=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import hydra\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from clearml import Task\n",
    "from omegaconf import OmegaConf\n",
    "from pytorch_lightning import seed_everything\n",
    "from pytorch_lightning.callbacks import (EarlyStopping, ModelCheckpoint,\n",
    "                                         ModelSummary, TQDMProgressBar)\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.datasets import (CausalLMDataset, CausalLMPredictionDataset,\n",
    "                          PaddingCollateFn)\n",
    "from src.metrics import Evaluator\n",
    "from src.models import SASRec\n",
    "from src.modules import SeqRec, SeqRecHuggingface\n",
    "from src.postprocess import preds2recs\n",
    "from src.prepr import last_item_split\n",
    "\n",
    "import itertools\n",
    "import pickle\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape (160178, 4)\n",
      "validation shape (66297, 4)\n",
      "test shape (70263, 4)\n",
      "Test global timepoint 1399939200.0\n",
      "Validation global timepoint 1394668800.0\n"
     ]
    }
   ],
   "source": [
    "if hasattr(config, 'cuda_visible_devices'):\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(config.cuda_visible_devices)\n",
    "\n",
    "\n",
    "train, validation, test, max_item_id, global_timepoint, global_timepoint_val = prepare_data(config)\n",
    "\n",
    "train_loader, eval_loader = create_dataloaders(train, validation, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12101"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_item_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 2728,   310,  7384,  ...,     0,     0,     0],\n",
      "        [ 3357,  9568,  2192,  ...,     0,     0,     0],\n",
      "        [ 9289, 10033, 12011,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 2102,  2983,   481,  ...,     0,     0,     0],\n",
      "        [   40,  3282,  9476,  ...,     0,     0,     0],\n",
      "        [ 2143,  1887,  4197,  ...,     0,     0,     0]]), 'user_id': tensor([  5,  11,  27,  37,  42,  46,  66,  69,  71,  75,  79,  82,  99, 102,\n",
      "        115, 123, 133, 144, 151, 153, 184, 198, 200, 205, 221, 231, 248, 257,\n",
      "        277, 289, 322, 333, 334, 343, 354, 358, 359, 360, 370, 375, 378, 387,\n",
      "        404, 419, 424, 438, 450, 479, 495, 497, 500, 502, 560, 565, 575, 576,\n",
      "        578, 580, 605, 616, 621, 622, 630, 633]), 'seen_ids': tensor([[ 2728,   310,  7384,  ...,     0,     0,     0],\n",
      "        [ 3357,  9568,  2192,  ...,     0,     0,     0],\n",
      "        [ 9289, 10033, 12011,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 2102,  2983,   481,  ...,     0,     0,     0],\n",
      "        [   40,  3282,  9476,  ...,     0,     0,     0],\n",
      "        [ 2143,  1887,  4197,  ...,     0,     0,     0]]), 'target': tensor([11283,  4348,  2403,  4443,  6395,  6143, 11415,  2049,  8383,  8146,\n",
      "        10386,  7245,   385,  9886,  5756,   470,  3029,  9392,  4646,  3611,\n",
      "         8291,  2872,  3414,  5430,  3769,  5652,  6054,  5743,  8095,  8192,\n",
      "         1201,  2807,  3646,    49,  9904,  1349,  6220,  6124,  4597,  1847,\n",
      "         1069,  6066,  1218,  2166, 10642,  1687, 10058,  6416,  9971,   877,\n",
      "         4191, 11943,  7201,  3475, 10293,  8163,  7353,  3490,  6288,  6682,\n",
      "         9040,  9720,  3177,  1847]), 'attention_mask': tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.]])}\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(eval_loader))\n",
    "\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/shares/SR003.nfs2/volodkevich/smiles_25/semantic_seqrec/notebooks'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from runs.train import create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(config, item_count=max_item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(12102, 64)\n",
       "    (wpe): Embedding(128, 64)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-1): 2 x GPT2Block(\n",
       "        (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=64, out_features=12102, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/shares/SR003.nfs2/volodkevich/smiles_25/semantic_seqrec/models/global_timesplit/val_by_time/Beauty/q09/GPT-2/\n"
     ]
    }
   ],
   "source": [
    "retrain = False\n",
    "split_subtype = config.split_subtype or ''\n",
    "q = 'q0' + str(config.quantile)[2:] if config.split_type == 'global_timesplit' else ''\n",
    "model_path = os.path.join(\n",
    "    os.path.dirname(os.path.abspath('.')), 'models', config.split_type,\n",
    "    split_subtype, config.dataset.name, q, config.model.model_class, 'retrain_with_val' if retrain else '')\n",
    "\n",
    "print(model_path)\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "if config.model.model_class == 'SASRec':\n",
    "    file_name = (\n",
    "        f\"{config.model.model_params.hidden_units}_\"\n",
    "        f\"{config.model.model_params.num_blocks}_\"\n",
    "        f\"{config.model.model_params.num_heads}_\"\n",
    "        f\"{config.model.model_params.dropout_rate}_\"\n",
    "        f\"{config.model.model_params.maxlen}_\"\n",
    "        f\"{config.dataloader.batch_size}_\"\n",
    "        f\"{config.random_state}\"\n",
    "    )\n",
    "elif config.model.model_class == 'GPT-2':\n",
    "    file_name = (\n",
    "        f\"{config.model.model_params.n_embd}_\"\n",
    "        f\"{config.model.model_params.n_layer}_\"\n",
    "        f\"{config.model.model_params.n_head}_\"\n",
    "        f\"{config.dataloader.batch_size}_\"\n",
    "        f\"{config.random_state}\"\n",
    "    )\n",
    "\n",
    "checkpoint_file = os.path.join(model_path, file_name + \".ckpt\")\n",
    "\n",
    "if config.model.model_class == 'GPT-2':\n",
    "    seqrec_module = SeqRecHuggingface(model, **config['seqrec_module'])\n",
    "else:   \n",
    "    seqrec_module = SeqRec(model, **config['seqrec_module']) \n",
    "\n",
    "model_summary = ModelSummary(max_depth=1)\n",
    "progress_bar = TQDMProgressBar(refresh_rate=20)\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    dirpath=model_path,  \n",
    "    filename='_' + file_name,           \n",
    "    save_top_k=1,\n",
    "    monitor=\"val_ndcg\",\n",
    "    mode=\"max\",\n",
    "    save_weights_only=True\n",
    ")\n",
    "early_stopping = EarlyStopping(monitor=\"val_ndcg\", mode=\"max\",\n",
    "                            patience=config.patience, verbose=False)\n",
    "callbacks = [early_stopping, model_summary, checkpoint, progress_bar]\n",
    "\n",
    "trainer = pl.Trainer(callbacks=callbacks, enable_checkpointing=True, \n",
    "                        **config['trainer_params'])\n",
    "\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/home/jovyan/shares/SR003.nfs2/volodkevich/seq_splits/av_env_seq_splits/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:653: Checkpoint directory /home/jovyan/shares/SR003.nfs2/volodkevich/smiles_25/semantic_seqrec/models/global_timesplit/val_by_time/Beauty/q09/GPT-2 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type            | Params\n",
      "------------------------------------------\n",
      "0 | model | GPT2LMHeadModel | 882 K \n",
      "------------------------------------------\n",
      "882 K     Trainable params\n",
      "0         Non-trainable params\n",
      "882 K     Total params\n",
      "3.531     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 318/318 [00:03<00:00, 92.11it/s, v_num=2, val_ndcg=0.0251, val_hit_rate=0.0439, val_mrr=0.0194]    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 318/318 [00:03<00:00, 91.46it/s, v_num=2, val_ndcg=0.0251, val_hit_rate=0.0439, val_mrr=0.0194]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model=seqrec_module,\n",
    "                    train_dataloaders=train_loader,\n",
    "                    val_dataloaders=eval_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.model.model_class == 'GPT-2':\n",
    "    if config.model.generation:\n",
    "        predict_dataset = CausalLMPredictionDataset(\n",
    "            test, max_length=config.dataset_params.max_length - max(config.evaluator.top_k))\n",
    "        \n",
    "        predict_loader = DataLoader(\n",
    "                predict_dataset, shuffle=False,\n",
    "                collate_fn=PaddingCollateFn(left_padding=True),\n",
    "                batch_size=config.dataloader.test_batch_size,\n",
    "                num_workers=config.dataloader.num_workers)\n",
    "        \n",
    "        seqrec_module.set_predict_mode(generate=True, mode=config.model.mode, **config.model.generation_params)\n",
    "\n",
    "    else:\n",
    "        predict_dataset = CausalLMPredictionDataset(test, max_length=config.dataset_params.max_length)\n",
    "\n",
    "        predict_loader = DataLoader(\n",
    "                predict_dataset, shuffle=False,\n",
    "                collate_fn=PaddingCollateFn(),\n",
    "                batch_size=config.dataloader.test_batch_size,\n",
    "                num_workers=config.dataloader.num_workers)\n",
    "        \n",
    "        seqrec_module.set_predict_mode(generate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 96/96 [00:07<00:00, 12.44it/s]\n"
     ]
    }
   ],
   "source": [
    "preds = trainer.predict(model=seqrec_module, dataloaders=predict_loader)\n",
    "recs = preds2recs(preds, successive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3797</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>5839</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2496</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>7966</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>180</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61095</th>\n",
       "      <td>22361</td>\n",
       "      <td>5730</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61096</th>\n",
       "      <td>22361</td>\n",
       "      <td>11689</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61097</th>\n",
       "      <td>22361</td>\n",
       "      <td>11178</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61098</th>\n",
       "      <td>22361</td>\n",
       "      <td>2876</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61099</th>\n",
       "      <td>22361</td>\n",
       "      <td>2999</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id  item_id  prediction\n",
       "0            0     3797    1.000000\n",
       "1            0     5839    0.500000\n",
       "2            0     2496    0.333333\n",
       "3            0     7966    0.250000\n",
       "4            0      180    0.200000\n",
       "...        ...      ...         ...\n",
       "61095    22361     5730    0.166667\n",
       "61096    22361    11689    0.142857\n",
       "61097    22361    11178    0.125000\n",
       "61098    22361     2876    0.111111\n",
       "61099    22361     2999    0.100000\n",
       "\n",
       "[61100 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(predict_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'early_stopping': False,\n",
       " 'num_return_sequences': 1,\n",
       " 'no_repeat_ngram_size': 1,\n",
       " 'do_sample': False}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqrec_module.generate_params['num_return_sequences'] = 1\n",
    "seqrec_module.generate_params['no_repeat_ngram_size'] = 1\n",
    "seqrec_module.generate_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFace Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = seqrec_module.model.generate(\n",
    "                batch['input_ids'][:, -seqrec_module.model.config.n_positions + seqrec_module.predict_top_k:].to(seqrec_module.model.device),\n",
    "                pad_token_id=seqrec_module.padding_idx,\n",
    "                max_new_tokens=seqrec_module.predict_top_k,\n",
    "                **seqrec_module.generate_params,\n",
    "            )\n",
    "\n",
    "preds = seq[:, -seqrec_module.predict_top_k:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     0,     0,  ...,   633,  3696, 11229],\n",
       "        [    0,     0,     0,  ...,  5883,  9882,  5403],\n",
       "        [    0,     0,     0,  ..., 12062,   856,   574],\n",
       "        ...,\n",
       "        [ 7666,  7831,  4680,  ..., 11951,  2335,  3539],\n",
       "        [    0,     0,     0,  ...,  9177,  1110,  6823],\n",
       "        [    0,     0,     0,  ..., 10735,  5754,  1547]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 28])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 38])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8096,   922,  5655,  6829,  4142, 10104, 11905,   633,  3696, 11229],\n",
       "        [ 7168,  5523,   332,  6993, 11678, 12070, 11460,  5883,  9882,  5403],\n",
       "        [ 7177,  1962,  5374,  5663,  3569,  1899,  8457, 12062,   856,   574],\n",
       "        [ 4565,  3575,  8645,   208,  9937,  7209,  2039,  4170,  8956, 10519],\n",
       "        [ 3315,  3869, 11681,  7729,  8355,  2382,   705,  2525,  2876, 11178],\n",
       "        [ 9806,  3468,  8160,  3669,  5265,  4737,  1022, 10786, 10762,   697],\n",
       "        [ 8443,  1292,   137,  4442, 11073,  1547,  3141,  8294,  2516,  7352],\n",
       "        [ 1682,  1566,  5940,  1547, 11073,  3141,   431,  9734,  7352,  5754],\n",
       "        [ 1890,  7280,  1727,  8411,   663,   456,  7551,  8851,  3772,  9714],\n",
       "        [ 7062,  3479, 10125, 11675, 11690,  2295,  5910,  8677, 11918,  2945],\n",
       "        [11451,  2331,  3103,   477,  2843, 10823,  5839,   107, 11665,  4756],\n",
       "        [ 6566,  4017,  3675,  5839,  2496, 12024, 10904,  9859,  7890, 11404],\n",
       "        [ 5117,  1375, 10378,  7735,  6221,  4674,  5891, 11143,  4340,  9777],\n",
       "        [ 5571,  9999, 11838,  1802,  7302,  5858,  7332,  4519,  4703,  7887],\n",
       "        [ 1035,  1551, 10045,  5630,  2536, 11927,  5892,  1727,  9289, 10723],\n",
       "        [11183,  6139,  7335,  4211, 11415, 11540,  6385, 10210,  5014,  6095],\n",
       "        [11073,  1547,  8294,  7352,  9734,  4442,  2564,  6644,  1447,  2516],\n",
       "        [ 1275,  6184, 10159,  5099,  3943,  8297,  3817,  4747, 11471,  4170],\n",
       "        [ 8443,  1292,  3141,  4442,  5754,  1547,  5554,  5259, 10739,  2564],\n",
       "        [ 3943,  5388,  8548,  3625, 10159, 11229,  1013,  7126,  3669,  8408],\n",
       "        [ 3664,  8998, 11989,  8357, 11440,  8203,  1276,  1646,  1737, 11153],\n",
       "        [10288,  6120, 10158,  1428,  7794, 10682,  9718,   657,  7600,  1760],\n",
       "        [ 6459,  5117,  1763,  4161,  2067, 10476,  5617,  4570, 10205,  4259],\n",
       "        [ 9251,  4863,  7402,  4056,  8291,  1236,  6083,  6818,  2335,  1645],\n",
       "        [ 4171,  9498, 11229,  1736,  4170, 12096, 10440,  4679,  4083,  6734],\n",
       "        [ 2291,  5142,   697,  3817,  6370, 11454, 11471,  4597, 11822, 11867],\n",
       "        [ 6891,  7458,  7931,  7149, 11822, 10985,  8068,  6551,   901,  4486],\n",
       "        [ 7862, 10762, 11518,  3664, 10786,  7295, 12014,  3468,  5142,   697],\n",
       "        [ 5627,  2627, 11477,  6101,  3943,  5099,   659,  1595,  6476,  6106],\n",
       "        [ 4403,  6406,  5655,  3152,   476,  9912, 10998,  2756, 10327,  9000],\n",
       "        [ 1890,  5233,  2619,  2335,  3980,  8143,   354,   976,  7890, 11404],\n",
       "        [  208,  8467,  7782,  1145,  3511,  1175,  4145, 10873,  2717,  8355],\n",
       "        [ 6476,    97,  3751,  8498,   477,  2843,  1959, 11131,  4592, 10139],\n",
       "        [ 3333,  8641,  8770,  7655,  5333,  2697,  8352,  1096,  6944, 12054],\n",
       "        [ 4150,  2039,  6692, 11390,  9191,  5648, 11947,  4737,  7586,  9490],\n",
       "        [  694,  3691,  6944,  8583, 12054,  4602, 11095, 11799,  3575,  2675],\n",
       "        [  898,  2655,  3333,  6113,  3741,  5265,  1736,  4170,  2521, 10519],\n",
       "        [ 7882,  7264,  7862, 10055,  8053,  1663,  3115,  9823,  4161,  4863],\n",
       "        [10386, 10975, 10473,  7667,  9562,  3949,  7613,  1736,  5265, 11229],\n",
       "        [  450, 10486,  6216,  9492,  5354, 10807,  7844,  7501,  5512,  1756],\n",
       "        [10883, 11485,  6692, 11489,  8796, 11862,  9227,  6838,  4658, 10409],\n",
       "        [ 9225,  9055, 11927,  6729,  3378,  3916,  9271,  2262,  9157,  4288],\n",
       "        [11947, 11835,  8425, 10092,  1216, 11984,  5648,  4173,  9552, 10742],\n",
       "        [ 9628,  8294, 11073,  1547,  7531,  5419, 10739, 10738,  1292,  8443],\n",
       "        [11501, 11661,  5922, 10144,  1675,  4945,   796,  5428,  2876, 11178],\n",
       "        [  426, 11689,  7782,  6986,  2049,  2896,  8975,  2729, 11507,  5412],\n",
       "        [ 4679, 10687,  6687, 11163,  1375, 11676,  5724,  6857,  6204,  6424],\n",
       "        [ 4259,  5117,  7264,  1887,   119,  1375,  4245,  5891,  5901,  6221],\n",
       "        [ 8232,  8677, 11918,   381, 12087,  6121,  3641,  4626,  1174,  9186],\n",
       "        [ 5265,  6370, 11454,  4599, 11799,  4170,  1334, 10028,  4565,  9912],\n",
       "        [ 8548,  5510,  9418,  7966,   180,  1861,  9675,  9198,  8774,  9859],\n",
       "        [ 5434,  5330,  6281,  6339,  3011,  6591,  8591,  1494,  3223,  8702],\n",
       "        [ 9289,  2049,  7890,  6406,  5839,  1962,  9418,  8271,  2952,  8674],\n",
       "        [ 9009,  9000, 10998,   250,  3664, 11150,  2717,  3886,  3801,  3580],\n",
       "        [ 8532, 11485,  5293, 10353,  5187,  5403, 10551,  8654, 10516, 11273],\n",
       "        [ 8443,  1292,  2564, 11073,  1547,  1569,  2624,  2900,  1447,  4442],\n",
       "        [ 8443,  1292,   431,  9734,  3225,  6292,  5300,   299, 11302,  8294],\n",
       "        [  461,  5031,  5711,  9279,  1398,  7005,  2179,  9173,  4737,  5265],\n",
       "        [ 2936,  2358,  5345,  1110, 11047,  3815,  8664,  5571,  3056,  3457],\n",
       "        [ 9251,  1768,  2104,  3003,  3850,  5955, 10122,  1918,  1003, 11113],\n",
       "        [ 3650,  6531,   661,   469,  4376,  5643,  8719, 11937, 10226,  9666],\n",
       "        [11003,  2619,  1406,   574,  2496,  5224,  8267, 11951,  2335,  3539],\n",
       "        [ 8419, 10805,  8467, 10850,  6863,  3815,  5345,  9177,  1110,  6823],\n",
       "        [  361,  6532,  5118,  8383,  1566,   431,  9284, 10735,  5754,  1547]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
