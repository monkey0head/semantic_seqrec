model_class: 'GPT-2'

pdrop: 0.1
model_params:
  n_positions: ${dataset_params.max_length} * ${semantic_ids_len}
  n_embd: 64 
  n_layer: 2
  n_head: 1
  embd_pdrop: ${model.pdrop}
  attn_pdrop: ${model.pdrop} 
  

generation: ${use_semantic_ids}
mode: 'greedy'
generation_params:
  # num_return_sequences: 1
  # no_repeat_ngram_size: 1
  do_sample: False
  # temperature: 0.05
  # num_beams: 10